---
title: "Supplementary Information 1"
subtitle: "Assessing ecosystem effects on the stock productivity of Antarctic Krill (*Euphausia superba*): An Integrated modeling perspective"
author: "Mardones, M; Jarvis Mason, E.T.; Pinones, A.;  Santa Cruz, F.; Cárdenas, C.A"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: SA_krill.bib
#csl: apa.csl
csl: icesjournal.csl
link-citations: yes
linkcolor: blue
output:
  bookdown::html_document2:
    keep_md: true
    toc: true
    toc_deep: 3
    toc_float:
      collapsed: false
      smooth_scroll: false
    theme: cosmo
    fontsize: 0.9em
    linestretch: 1.7
    html-math-method: katex
    self-contained: true
    code-tools: true
    number_sections: false
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup1, echo=FALSE}
#rm(list = ls())
#set.seed(999)
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      fig.height = 6,
                      fig.width = 7,
                      fig.align = 'center',
                      fig.pos = "H",
                      dev = 'jpeg',
                      fig.path = "Figs/",
                      dpi = 300,
                      tidy.opts=list(width.cutoff=50), 
                      tidy=TRUE)
#XQuartz is a mess, put this in your onload to default to cairo instead
options(bitmapType = "cairo") 
# (https://github.com/tidyverse/ggplot2/issues/2655)
# Lo mapas se hacen mas rapido
```


```{r message=FALSE, eval=TRUE}
# install.packages("devtools")
# devtools::install_github("r4ss/r4ss", ref="development")
# install.packages("caTools")
# library("caTools")
# install.packages("r4ss")
library(r4ss)
library(here)
#remotes::install_github("PIFSCstockassessments/ss3diags")
library(ss3diags)
library(kableExtra)
library(doParallel) # facilita la ejecución paralela en R
detectCores()
registerDoParallel(8)
library(ggpubr)
library(tibble)
library(openxlsx)
library(ggthemes)
library(forecast)
library(tidyr)
library(mixR)
library(readxl)
library(tidyverse)
library(ggridges)
library(ggrepel)
```

```{r message=FALSE, warning=FALSE, echo=TRUE}
# dir01 <- here("s01") # agreggate data (no spatial diferences)
# dir1<-here("s1") # Data strata flishery
# dir2<-here("s2") # Same 9 with areas (SubStrata) as fleet. Dif size comoposition and dif CPUE and dif survey length and biomass data by strata
# dir3<-here("s3") # without S-R
# dir4<-here("s4") # 
# dir5<-here("s5") # 
# dir6<-here("s6") # 
# dir7<-here("s7") # 2 set parametres EMM-2024/23 (Mardones)
# dir8<-here("s8") # s1 platoons
# dir9<-here("s9") # s1 w/ blocks
dir1.1<-here("s1.1") # sin predador ni ambiental
dir1.2 <- here("s1.2") # s1.1 C/ predador
dir1.3 <- here("s1.3") # s1.1 solo env
dir1.4 <- here("s1.4") # s1.2 predator and env
Figs <- here("Figs")# S
```


# OVERVIEW

This study aims to evaluate the impact of ecosystem components—such as environmental variables and predator-prey interactions—on the productivity and key population dynamics of *Euphausia superba* in Subarea 48.1. By incorporating these factors into the assessment, we analyze how krill population variables respond to ecological variability, providing insights into their resilience and potential management implications.  

Here, the **reference model** represents a baseline assessment of *Euphausia superba* population dynamics in Subarea 48.1, excluding environmental and ecological variables. This model assumes that krill productivity and population parameters are driven  by intrinsic biological processes, such as growth, mortality, and recruitment and fishery impacts without accounting for external influences like environmental variability or predation pressure. By serving as a *control scenario*, this model provides a benchmark against which the impact of ecosystem components in productivity can be evaluated, allowing for a direct comparison of how environmental and ecological factors influence krill stock dynamics.  

The study area corresponds to Subarea 48.1 in the WAP, where most krill fishing activity occurs and where CCAMLR has been working to implement a new management strategy at a reduced spatial scale (Figure \@ref(fig:mapa)). To refine the spatial resolution of krill population dynamics and analysis, we adopted the five strata proposed by CCAMLR: Bransfield Strait, Elephant Island, Gerlache Strait, Joinville Island, and the Southwest [@Dornam2021; REF] (Fig 1 S1). While these strata were originally designed for management purposes without considering specific biological or population characteristics, their subdivision enables us to analyze the data at a finer scale. This level of detail facilitates the identification of heterogeneity across various data sources, whether these originate from the fishery or from environmental survey data, providing a more nuanced understanding of krill population dynamics. This spatial segregation of information is incorporated into the stock assessment model, making it a spatially explicit model that accounts for regional variability in population dynamics and external factors.

```{r mapa, out.width='50%', fig.cap="Subarea 48.1 and management strata considered in the spatio-temporal analysis of intrinsic productivity of Krill (BS=Brainsfield Strait, EI= Elephant Island, Gerlache= Gerlache strait, JOIN= Joinville Island, SSWI= South West)"}
knitr::include_graphics('Figs/481.png')
```

In this approach, spatial structure is incorporated implicitly by treating different areas as separate fleets [@Nielsen2021; @Waterhouse2014]. This constitutes a spatially implicit modeling framework, where differences among strata are recognized both from the perspective of krill population dynamics and from the influence of environmental variability within Subarea 48.1 (Figure \@ref(fig:concem)). 

```{r concem, out.width='40%', fig.cap="Conceptual model used to model dynamics population in Antarctic krill in WAP"}
knitr::include_graphics('Figs/conceptual.jpeg')
```

## Statistical Model (`SS3`)

Stock Synthesis (v.3.30.21)  is a widely used tool for assessing fish and invertebrate populations, including Antarctic krill. SS3 is implemented in `C++` with estimation enabled through automatic differentiation (ADMB) [@Fournier2012; @Methot2013]. In this exercise, SS3 is configured as an integrated stock assessment model, explicitly accounting for age and size structure while incorporating key ecosystem drivers. The model simulates population processes such as growth, maturity, fecundity, recruitment, movement, and mortality, while also integrating environmental variability and predator-prey relationships to refine estimates of population trends. The analysis of model outputs is conducted using R, utilizing the *r4ss* and *ss3diags* packages [@Taylor2019; @Winker2023]. By leveraging a spatially implicit, ecosystem-informed approach, this assessment provides a robust framework for evaluating krill stock dynamics under changing environmental conditions. These insights are crucial for informing sustainable management strategies in the Antarctic Peninsula region, where krill plays a foundational role in the marine food web.  


```{r path, fig.cap="Framework path to stock assessment model in krill in WAP (Yellow boxes is not implemeted yet)."}
knitr::include_graphics('Figs/pathmod.png')
```

## Parameters

The following table summarizes the key parameters to conditioning the reference model, including biological, growth, and population dynamics factors.

```{r}
start1 <- SS_readstarter(file = file.path(dir1.4,
                                          "starter.ss"),
                              verbose = FALSE)
dat1 <- SS_readdat(file = file.path(dir1.4, start1$datfile),
                        verbose = FALSE)
ctl1 <-  r4ss::SS_readctl(file = file.path(dir1.4,
                                    start1$ctlfil),
                        verbose = FALSE,
                        use_datlist = TRUE, 
                   datlist = dat1)
fore1 <- r4ss::SS_readforecast(file = file.path(dir1.4, 
                                                "forecast.ss"),
                              verbose = FALSE)
# can also read in wtatage.ss for an empirical wt at age model using
# r4ss::SS_readwtatage()
```


```{r}
parbio<-ctl1$MG_parms[1:10,c(1:3,7)]
 row.names( parbio)<-c("Nat M",
                       "Lmin", 
                       "Lmax",
                       "VonBert K",
                       "CV young",
                       "CV old", 
                       "Wt a", 
                       "Wt b",
                       "L50%", 
                       "Mat slope")

 SRpar<-ctl1$SR_parms[1:5,c(1:3,7)]
 Qpar<-ctl1$Q_parms[1:2,c(1:3,7)]
 Selpar<-ctl1$size_selex_parms[1:22,c(1:3,7)]
 parInit<-as.data.frame(rbind(parbio,SRpar,Qpar,Selpar))

wb <- createWorkbook()

addWorksheet(wb, "parameters")
writeData(wb, "parameters", parInit)

# Guardar el workbook
#saveWorkbook(wb, "DataKrill.xlsx", overwrite = TRUE)

```

```{r}
parInit %>%
  mutate(across(where(is.numeric), ~ round(., 3)))  %>% 
  kbl(booktabs = TRUE,
      format = "html",
      position = "ht!",
      caption = "Input parameters for the initial SS3 model of krill. Each parameter line contains a minimum value (LO), maximum value (HI), and initial value (INIT). If the phase (PHASE) for the parameter is negative, the parameter is fixed as input") %>%
  kable_paper("hover", full_width = TRUE) %>%  # Expande el ancho
  kable_styling(latex_options = c("striped", "condensed"),
                full_width = TRUE,  # Expande la tabla al ancho completo
                font_size = 12,
                html_font = "Times New Roman") %>%  # Aplica la fuente Times New Roman
  pack_rows(index = c("Natural Mortality" = 1,
                      "Growth" = 5,
                      "relationship Length-Weigth" = 2,
                      "Maturity" = 2,
                      "S-R relation" = 5,
                      "Catchability" = 2,
                      "Selectivity" = 4))

```
Source of data input.

Ibdex abundance in Figure \@ref(fig:index)

```{r}
dat1_std <- dat1$CPUE %>%
  mutate(
    index_std = (obs - min(obs)) / (max(obs) - min(obs)),
    index = recode(index,
                   `1` = "FISHERYBS",
                   `2` = "FISHERYEI",
                   `3` = "FISHERYGS",
                   `4` = "FISHERYJOIN",
                   `5` = "FISHERYSSIW",
                   `6` = "SURVEYBS",
                   `7` = "SURVEYEI",
                   `8` = "SURVEYGS",
                   `9` = "SURVEYJOIN",
                   `10` = "SURVEYSSIW",
                   `11` = "PREDATOR"),
    index = factor(index, levels = c("FISHERYBS", 
                                     "FISHERYEI", 
                                     "FISHERYGS",
                                     "FISHERYJOIN", 
                                     "FISHERYSSIW", 
                                     "SURVEYBS", 
                                     "SURVEYEI", 
                                     "SURVEYGS", 
                                     "SURVEYJOIN", 
                                     "SURVEYSSIW", 
                                     "PREDATOR")),
    category = case_when(
      index %in% c("FISHERYBS", "FISHERYEI", "FISHERYGS", "FISHERYJOIN", "FISHERYSSIW") ~ "Fishery",
      index %in% c("SURVEYBS", "SURVEYEI", "SURVEYGS", "SURVEYJOIN", "SURVEYSSIW") ~ "Survey",
      index == "PREDATOR" ~ "Predator",
      TRUE ~ NA_character_  
    )
  )
```


```{r index, fig.width=9, fig.height=4, fig.cap="Standardized indices of krill index abundance and consumption from fishery-dependent, fishery-independent, and predator-based data sources across different strata within Subarea 48.1. Each panel represents a distinct spatial or functional stratum, with trend lines indicating temporal variation from 1990 to 2020. Colors denote data source categories: green for fishery, orange for scientific surveys, and purple for predator-based indices. These patterns highlight spatial and temporal heterogeneity in krill dynamics across the subarea."}

p1 <- ggplot(dat1_std, aes(x = year, y = index_std, group = index, color = category)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~index, scales = "free_y", ncol = 5) +  # Ajusta ncol para obtener 3 filas
  scale_color_manual(values = c("Fishery" = '#1b9e77', 
                                "Survey" = '#d95f02',
                                "Predator" = '#7570b3')) + 
  theme_bw() +
  theme(legend.position = "top",
    axis.text.x = element_text(angle = 90, hjust = 1),
         axis.text.y = element_blank()) +
  labs(title = "", x = "", y = "Standard index", color = "Category")
p1
```


```{r warning=FALSE}
dat_long <- dat1$lencomp %>% 
  pivot_longer(cols = starts_with("l"),  
               names_to = "Talla",  
               values_to = "Frecuencia") %>% 
  mutate(Talla = as.numeric(sub("l", "", Talla))) %>%  
  mutate(FltSvy = recode(FltSvy,
                         `1` = "FISHERYBS",
                         `2` = "FISHERYEI",
                         `3` = "FISHERYGS",
                         `4` = "FISHERYJOIN",
                         `5` = "FISHERYSSIW",
                         `6` = "SURVEYBS",
                         `7` = "SURVEYEI",
                         `8` = "SURVEYGS",
                         `9` = "SURVEYJOIN",
                         `10` = "SURVEYSSIW",
                         `11` = "PREDATOR")) %>%
  mutate(FltSvy = factor(FltSvy, levels = c("FISHERYBS", 
                                            "FISHERYEI", 
                                            "FISHERYGS",
                                            "FISHERYJOIN", 
                                            "FISHERYSSIW",
                                            "SURVEYBS",
                                            "SURVEYEI", 
                                            "SURVEYGS", 
                                            "SURVEYJOIN",
                                            "SURVEYSSIW",
                                            "PREDATOR"))) 

mean_talla <- dat_long %>%
  group_by(FltSvy) %>%
  summarise(mean_Talla = mean(Talla, na.rm = TRUE))


dat_expanded <- dat_long %>%
  uncount(weights = Frecuencia)
```

Length compositions in Figure \@ref(fig:length)

```{r length, fig.width=10, fig.height=4, warning=FALSE, fig.cap="Annual length-frequency distributions of Antarctic krill (Euphausia superba) across different data sources and spatial strata within Subarea 48.1 from 1991 to 2020. Each panel represents a distinct stratum for either fishery-dependent (green), fishery-independent survey (orange), or predator-based (purple) observations. Density ridgelines illustrate variation in krill size structure across years. The red vertical line marks a recruit references length (3.6 cm)."}
p2 <- ggplot(dat_expanded, aes(x = Talla, y = as.factor(Yr), fill = FltSvy)) +
  geom_density_ridges_gradient(scale = 1.8,
                               rel_min_height = 0.001,
                               col = "black",
                               alpha=0.4) +
  facet_wrap(~FltSvy, ncol = 10) +
  geom_vline(data = mean_talla, aes(xintercept = mean_Talla), 
             color = "red", 
             size = 0.7,
             alpha = 0.5) +
  scale_fill_manual(values = c("FISHERYBS" = '#1b9e77',  
                               "FISHERYEI" = '#1b9e77',  
                               "FISHERYGS" = '#1b9e77',
                               "FISHERYJOIN" = '#1b9e77',  
                               "FISHERYSSIW" = '#1b9e77',  
                               "SURVEYBS" = '#d95f02',  
                               "SURVEYEI" = '#d95f02',  
                               "SURVEYGS" = '#d95f02',  
                               "SURVEYJOIN" = '#d95f02',  
                               "SURVEYSSIW" = '#d95f02',  
                               "PREDATOR" = '#7570b3')) +  
  labs(title = "",
       x = "cm",
       y = "",
       fill = "Category") +
  theme_bw()+
  theme(legend.position = "none")
p2
```



## Scenarios

In Table 1 we have ten scenarios to test different option in modeling
about main consideration in assessment of krill population.

| Scenario | Description                                                                          |
|:------------:|:---------------------------------------------------------|
|    s1.1  | Spatial data without environmental and predator components                           |
|    s1.2  | "s1.1" with predator components                                                      |
|    s1.3  | "s1.1" with environmental variable                                                  |
|    s1.4  | "s1.1" w/ both, predator fleet and environmental variable                           |



### Run Models

```{r eval=FALSE, message=F, include=FALSE, echo=TRUE}
directorios <- c("s1.1",
                 "s1.2",
                 "s1.3",
                 "s1.4")  

for (dir in directorios) {
  r4ss::run(
    dir = dir,
    exe = "ss_osx",
    skipfinished = FALSE,
    show_in_console = TRUE
  )
}
```

```{r eval=FALSE, message=F, include=FALSE}
#OR with rby separate
#shell(cmd="ss") # run SS to windows
# or 
r4ss::run(
  dir = dir1.1,
  exe = "ss_osx",
  skipfinished = FALSE, 
  show_in_console = TRUE 
)


# s1.2
r4ss::run(
  dir = dir1.2,
  exe = "ss_osx",
  skipfinished = FALSE, 
  show_in_console = TRUE 
)


# s1.3
r4ss::run(
  dir = dir1.3,
  exe = "ss_osx",
  skipfinished = FALSE, 
  show_in_console = TRUE 
)


# s1.4
r4ss::run(
  dir = dir1.4,
  exe = "ss_osx",
  skipfinished = FALSE, 
  show_in_console = TRUE 
)
```



```{r message=F, include=FALSE}
#s1.1
base.model1.1 <- SS_output(dir=dir1.1,
                        covar=T,
                        forecast=T)
#s1.2
base.model1.2 <- SS_output(dir=dir1.2,
                        covar=T,
                        forecast=T)
#s1.3
base.model1.3 <- SS_output(dir=dir1.3,
                        covar=T,
                        forecast=T)
#s1.4
base.model1.4 <- SS_output(dir=dir1.4,
                        covar=T,
                        forecast=T)

```


Data used en both (spatial and No spatial models) `s1.1` and `s1.4` to compare

```{r fig.height=10, fig.width=6}
par(mfrow=c(2,1), mar=c(5,4,2,1))
SSplotData(base.model1.1, 
           subplots = 1,
           ptsize = 0.5)

SSplotData(base.model1.4, 
           subplots = 1,
           ptsize = 0.5)
```

# RESULTS

```{r eval=FALSE, message=F, include=FALSE}
# Definir la ruta de la carpeta
folder_path <- "s1.1/plots"
# Verificar si la carpeta existe y eliminarla
if (dir.exists(folder_path)) {
  unlink(folder_path, recursive = TRUE)
  message("La carpeta 'plots' ha sido eliminada.")
} else {
  message("La carpeta 'plots' no existe.")
}

SS_plots(base.model1.1, 
         uncertainty = TRUE,
         datplot = T, 
         png=T, 
         aalresids = F,
         btarg=0.75, 
         minbthresh=0.2, 
         forecast=T)

# S 1.2

# Definir la ruta de la carpeta
folder_path2 <- "s1.2/plots"
# Verificar si la carpeta existe y eliminarla
if (dir.exists(folder_path2)) {
  unlink(folder_path2, 
         recursive = TRUE)
  message("La carpeta 'plots' ha sido eliminada.")
} else {
  message("La carpeta 'plots' no existe.")
}

SS_plots(base.model1.2, 
         uncertainty = TRUE,
         datplot = T, 
         png=T, 
         aalresids = F,
         btarg=0.75, 
         minbthresh=0.2, 
         forecast=T)

# S 1.3

# Definir la ruta de la carpeta
folder_path3 <- "s1.3/plots"
# Verificar si la carpeta existe y eliminarla
if (dir.exists(folder_path3)) {
  unlink(folder_path3, 
         recursive = TRUE)
  message("La carpeta 'plots' ha sido eliminada.")
} else {
  message("La carpeta 'plots' no existe.")
}

SS_plots(base.model1.3, 
         uncertainty = TRUE,
         datplot = T, 
         png=T, 
         aalresids = F,
         btarg=0.75, 
         minbthresh=0.2, 
         forecast=T)


# S 1.4

# Definir la ruta de la carpeta
folder_path4 <- "s1.4/plots"
# Verificar si la carpeta existe y eliminarla
if (dir.exists(folder_path4)) {
  unlink(folder_path4, 
         recursive = TRUE)
  message("La carpeta 'plots' ha sido eliminada.")
} else {
  message("La carpeta 'plots' no existe.")
}

SS_plots(base.model1.4, 
         uncertainty = TRUE,
         datplot = T, 
         png=T, 
         aalresids = F,
         btarg=0.75, 
         minbthresh=0.2, 
         forecast=T)
```


Main Variables poulation in `s1.1` scenario

```{r fig.height=2, fig.width=7}
par(mfrow=c(1,3), mar=c(5,4,1,1)) 
SSplotTimeseries(base.model1.1,
                 subplot = 1,
                 xlab = "")
SSplotTimeseries(base.model1.1,
                 subplot = 9,
                 xlab = "")
SSplotTimeseries(base.model1.1,
                 subplot = 7,
                 xlab = "")
```

Main Variables poulation in `s1.2` scenario

```{r fig.height=2, fig.width=7}
par(mfrow=c(1,3), mar=c(5,4,1,1))
SSplotTimeseries(base.model1.2,
                 subplot = 1,
                 xlab = "")
SSplotTimeseries(base.model1.2,
                 subplot = 9,
                 xlab = "")
SSplotTimeseries(base.model1.2,
                 subplot = 7,
                 xlab = "")
```

Main Variables poulation in `s1.3` scenario

```{r fig.height=2, fig.width=7}
par(mfrow=c(1,3), mar=c(5,4,1,1))
SSplotTimeseries(base.model1.3,
                 subplot = 1,
                 xlab = "")
SSplotTimeseries(base.model1.3,
                 subplot = 9,
                 xlab = "")
SSplotTimeseries(base.model1.3,
                 subplot = 7,
                 xlab = "")
```


Main Variables poulation in `s1.4` scenario

```{r fig.height=2, fig.width=7}
par(mfrow=c(1,3), mar=c(5,4,1,1))
SSplotTimeseries(base.model1.4,
                 subplot = 1,
                 xlab = "")
SSplotTimeseries(base.model1.4,
                 subplot = 9,
                 xlab = "")
SSplotTimeseries(base.model1.4,
                 subplot = 7,
                 xlab = "")
```
Selectivity

```{r}
par(mfrow=c(2,2), mar=c(5,4,1,1))
SSplotSelex(base.model1.1,
            subplots = 1,
            mainTitle = FALSE)
mtext("s1.1", side=3, line=0.5, cex=1.2)
SSplotSelex(base.model1.2,
            subplots = 1,
            mainTitle = FALSE)
mtext("s1.2", side=3, line=0.5, cex=1.2)
SSplotSelex(base.model1.3,
            subplots = 1,
            mainTitle = FALSE)
mtext("s1.3", side=3, line=0.5, cex=1.2)
SSplotSelex(base.model1.4,
            subplots = 1,
            mainTitle = FALSE)
mtext("s1.4", side=3, line=0.5, cex=1.2)
```
```{r}
par(mfrow=c(2,2), mar=c(5,4,1,1))
SSplotBiology(base.model1.1,
            subplots = 1)
SSplotBiology(base.model1.2,
            subplots = 1)
SSplotBiology(base.model1.3,
            subplots = 1)
SSplotBiology(base.model1.4,
            subplots = 1)
```

Natural Mortality

```{r eval=FALSE}
par(mfrow=c(4,2), mar=c(5,4,2,1))
SSplotBiology(base.model1.1,
              subplots = 21)
SSplotBiology(base.model1.2,
            subplots = 21)
mtext("s1.2", side=3, line=0.5, cex=1.2)
SSplotBiology(base.model1.3,
            subplots = 21)
mtext("s1.3", side=3, line=0.5, cex=1.2)
SSplotBiology(base.model1.4,
            subplots = 21)
mtext("s1.4", side=3, line=0.5, cex=1.2)
```

```{r}
par(mfrow=c(2,2), mar=c(5,4,2,1)) 

SSplotIndices(base.model1.1, subplots = 9)
mtext("s1.1", side=3, line=0.5, cex=1.2)

SSplotIndices(base.model1.2, subplots = 9)
mtext("s1.2", side=3, line=0.5, cex=1.2)

SSplotIndices(base.model1.3, subplots = 9)
mtext("s1.3", side=3, line=0.5, cex=1.2)

SSplotIndices(base.model1.4, subplots = 9)
mtext("s1.4", side=3, line=0.5, cex=1.2)

```

```{r}
SSplotComps(base.model1.1,
            subplots = 21)
SSplotComps(base.model1.2,
            subplots = 21)
SSplotComps(base.model1.3,
            subplots = 21)
SSplotComps(base.model1.4,
            subplots = 21)
```
Total biomass

```{r}
total1 <- as.data.frame(base.model1.1$timeseries[,c(2,5)]) %>%
  mutate(Serie = "Without Ecosystem Variables")
total2 <- as.data.frame(base.model1.2$timeseries[,c(2,5)]) %>% 
  mutate(Serie = "W/ Predator")
total3 <- as.data.frame(base.model1.3$timeseries[,c(2,5)]) %>% 
  mutate(Serie = "W/ Environment")
total4 <- as.data.frame(base.model1.4$timeseries[,c(2,5)]) %>% 
  mutate(Serie = "With Predator and Ecosystem")


totalbio <- rbind(total1,
              total2,
              total3,
              total4)

ggplot(totalbio, aes(x = Yr, y = Bio_all, color=Serie)) +
  geom_point() +
  geom_smooth(method = "loess",
              span = 0.5,
              se = FALSE) +
  labs(title = "",
       x = "",
       y = "Total biomass (mt)") +
  theme_few() +
  scale_color_manual(name = "",
                     values = c("Without Ecosystem Variables" = "#ca0020",  
                                "W/ Predator" = "#f4a582",  
                                "W/ Environment" = "#bababa",  
                                "With Predator and Ecosystem" = "#404040")) +
  theme(axis.text.x = element_text(angle = 90, 
                                   hjust = 1))

```

Pearson residual

```{r}
create_heatmap_df <- function(df, model_name) {
  df %>%
    dplyr::filter(Pearson < 5) %>%
    dplyr::select(c(1, 6, 19, 16)) %>%
    dplyr::mutate(Fleet = dplyr::case_when(
      Fleet == 1 ~ "FISHERYBS",
      Fleet == 2 ~ "FISHERYEI",
      Fleet == 3 ~ "FISHERYGS",
      Fleet == 4 ~ "FISHERYJOIN",
      Fleet == 5 ~ "FISHERYSSIW",
      Fleet == 6 ~ "SURVEYBS",
      Fleet == 7 ~ "SURVEYEI",
      Fleet == 8 ~ "SURVEYGS",
      Fleet == 9 ~ "SURVEYJOIN",
      Fleet == 10 ~ "SURVEYSSIW",
      Fleet == 11 ~ "PREDATOR",
      TRUE ~ NA_character_
    )) %>%
    dplyr::mutate(Model = model_name)
}

# Unir los datasets en uno solo
df_all <- bind_rows(
  create_heatmap_df(base.model1.1$lendbase, "Model 1.1"),
  create_heatmap_df(base.model1.2$lendbase, "Model 1.2"),
  create_heatmap_df(base.model1.3$lendbase, "Model 1.3"),
  create_heatmap_df(base.model1.4$lendbase, "Model 1.4")
)

# Plot combinado: facet_grid por Modelo y Fleet
heatmapres <- ggplot(df_all, aes(x = Yr, y = Bin, fill = Pearson)) +
  geom_tile() +
  scale_fill_gradient2(low = "red", 
                       mid = "white", 
                       high = "blue",
                       midpoint = 0) +
  facet_grid(Fleet ~ Model) +
  theme_few() +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1, size = 5),         
    axis.text.y = element_text(size = 5),                               
    strip.text = element_text(size = 8),                                 
    legend.position = "none"
  ) +
  labs(x = "", y = "Length (cm)", fill = "Pearson")

heatmapres
```


This figure shows the Pearson residuals of predicted length distributions for krill across four modeling scenarios, each incorporating different levels of ecosystem complexity. The residuals are visualized by year and length bin, allowing an assessment of model fit over time and across the size structure of the population. In the "Without Ecosystem Variables" scenario (top left), substantial lack of fit is evident across multiple years and length classes, particularly in the late 1990s and early 2000s. Positive Pearson residuals indicate that the model tends to underestimate frequencies of certain length bins, especially among smaller and mid-sized individuals, suggesting poor representation of recruitment and growth dynamics in the absence of environmental and predator effects. The "With Predator" scenario (top right) shows some improvement in model fit, with slightly more balanced residuals. However, there are still consistent deviations in the mid-size ranges across multiple years, indicating that predation alone does not fully explain observed variability in length composition. In the "With Environment" scenario (bottom left), there is a noticeable reduction in residual magnitude across several years and size classes, implying that incorporating environmental covariates helps align the model more closely with observed size distributions. This is particularly apparent in the early 2000s, where the residuals are closer to zero across most bins. Finally, the "With Predator and Ecosystem" scenario (bottom right) shows the most consistent reduction in Pearson residuals across time and size structure. The distribution of residuals is more homogeneous and centered around zero, indicating that the combined inclusion of predator and environmental drivers leads to the best model fit among the four scenarios.

Overall, the results support the conclusion that ecosystem-informed models better capture the temporal and size-structured dynamics of krill, with the **joint inclusion of predator and environmental variables** yielding the most robust representation of observed length compositions.


## Diagnosis Base Model

A rigorous model diagnosis is essential to ensure the reliability and robustness of stock assessment models. The key steps for a good practice in model diagnosis include:  

1. Convergence Check: The model must reach a final convergence criterion of 1.0e-04 to ensure numerical stability and reliable parameter estimation.  

2. Residual Analysis: Both visual inspection and statistical metrics are used to evaluate model residuals, helping to detect patterns of bias or misfit.  

3. Retrospective Analysis: The Mohn’s rho parameter is used to assess the consistency of model estimates when sequentially removing recent years of data, identifying potential overestimation or underestimation trends.  

4. Likelihood Profile Analysis: This approach examines how the likelihood function behaves across a range of parameter values, providing insight into parameter uncertainty and model sensitivity.  

This framework follows the recommendations outlined by @Carvalho2021b, aiming to enhance transparency and reproducibility in model evaluation.

### Convergence Criteria

The convergence criterion used for model calibration is set to a final threshold of **0.0001** (or equivalently **1.0e-04**). This criterion defines the minimum acceptable difference between successive model iterations. Convergence is considered achieved when the absolute change in the objective function value or key parameters falls below this threshold. A smaller convergence value ensures that the model achieves a high degree of accuracy and stability in its final estimates, indicating that further iterations are unlikely to result in significant changes to the parameter estimates.

### Residual consistency 


Residual analysis is a critical component of model diagnostics in stock assessments. It helps evaluate the fit of the model to observed data and detect potential biases or inconsistencies. This process is applied to both length composition data and abundance indices such as CPUE (Catch Per Unit Effort) and survey-derived estimates.  

For length composition data, residuals represent the difference between observed and model-predicted length distributions. The standardized residuals are calculated as the difference between observed and expected proportions at each length bin. These residuals are plotted by year to identify systematic trends, biases, or inconsistencies in the data. Ideally, they should be randomly distributed around zero, indicating no systematic over- or underestimation.  

For abundance indices such as CPUE and fishery-independent surveys, residuals are analyzed to assess model fit and potential sources of bias. Residuals are computed as the difference between observed index values and those predicted by the model, typically standardized by dividing by the standard error to facilitate comparison across years. These residuals are then plotted over time to evaluate trends. A shaded confidence region, like the green area in the provided plot, represents expected variability, with outliers highlighted in red or other distinct markers. Persistent positive or negative residuals may indicate systematic bias in the model or data collection process.  

Statistical diagnostics are also performed to check for autocorrelation in residuals, which can indicate potential model misspecifications. When mean residual values are close to zero, the model fit is considered unbiased.  

By integrating these residual analyses for both length and abundance indices, stock assessment models can be refined, improving their reliability and increasing confidence in the assessment results.

```{r}
par(mfrow=c(2,4), mar=c(5,4,2,1)) 
SSplotRunstest(base.model1.1,
               subplots = "len",
               add=T,
               plot = TRUE,
               plotdir = Figs)
par(mfrow=c(2,5), mar=c(5,4,2,1)) 
SSplotRunstest(base.model1.2,
               subplots = "len",
               add=T,
               plot = TRUE,
               plotdir = Figs)
par(mfrow=c(2,4), mar=c(5,4,2,1)) 
SSplotRunstest(base.model1.3,
               subplots = "len",
               add=T,
               plot = TRUE,
               plotdir = Figs)
par(mfrow=c(2,5), mar=c(5,4,2,1)) 
SSplotRunstest(base.model1.4,
               subplots = "len",
               add=T,
               plot = TRUE,
               plotdir = Figs)
```


```{r}
par(mfrow=c(2,5), mar=c(5,4,2,1))
SSplotRunstest(base.model1.1,
               subplots = "cpue",
               add=T,
               plot = TRUE,
               plotdir = Figs)
par(mfrow=c(2,5), mar=c(5,4,2,1))
SSplotRunstest(base.model1.2,
               subplots = "cpue",
               add=T,
               plot = TRUE,
               plotdir = Figs)
par(mfrow=c(2,5), mar=c(5,4,2,1))
SSplotRunstest(base.model1.3,
               subplots = "cpue",
               add=T,
               plot = TRUE,
               plotdir = Figs)
par(mfrow=c(2,5), mar=c(5,4,2,1))
SSplotRunstest(base.model1.4,
               subplots = "cpue",
               add=T,
               plot = TRUE,
               plotdir = Figs)
```


### Residual Analysis and RMSE 

Residual analysis of mean length data is a fundamental diagnostic tool in stock assessments. It helps evaluate whether the model provides an unbiased fit to the observed data and detects potential biases over time. In this figure, mean length residuals are plotted across years, differentiated by data source, including fishery-dependent (FISHERY) and fishery-independent (SURVEY) datasets, as well as predator-related observations (PREDATOR). The residuals represent the deviation of observed mean length from model-predicted values, standardized to facilitate interpretation.  

The black line represents a locally estimated scatterplot smoothing (Loess) curve, which provides a trend line to visualize systematic deviations over time. The presence of persistent positive or negative trends in the residuals may indicate biases in the growth model, selectivity assumptions, or misrepresentation of recruitment variability. The gray bars highlight periods where residual variability is particularly high, suggesting potential inconsistencies between observed and predicted size structures.  

RMSE quantifies the overall deviation between observed and predicted values, providing an aggregate measure of model fit. Lower RMSE values indicate better agreement between observed and predicted data. In fisheries stock assessment [@HurtadoF2015], RMSE thresholds for acceptable model performance typically range between *10% and 30%*, depending on the data quality and complexity of the population dynamics being modeled. Values exceeding this range suggest potential biases, requiring further investigation into the model structure, parameter estimation, or data sources.   

By analyzing residual patterns and RMSE values, the model can be refined to improve the accuracy of mean length predictions, ultimately enhancing the reliability of stock assessment outcomes and management recommendations.

```{r}
par(mfrow=c(2,2), mar=c(5,4,2,1))
rmse1l <- SSplotJABBAres(base.model1.1,
               subplots = "len",
               add=T)
rmse2l <- SSplotJABBAres(base.model1.2,
               subplots = "len",
               add=T)
rmse3l <- SSplotJABBAres(base.model1.3,
               subplots = "len",
               add=T)
rmse4l <- SSplotJABBAres(base.model1.4,
               subplots = "len",
               add=T)
```

```{r}
par(mfrow=c(2,2), mar=c(5,4,2,1))
rmse1c <- SSplotJABBAres(base.model1.1,
               subplots = "cpue",
               add=T)
rmse2c <- SSplotJABBAres(base.model1.2,
               subplots = "cpue",
               add=T)
rmse3c <- SSplotJABBAres(base.model1.3,
               subplots = "cpue",
               add=T)
rmse4c <- SSplotJABBAres(base.model1.4,
               subplots = "cpue",
               add=T)
```

another

```{r}
dfpearson_long <- data.frame(
  Pearson = c(base.model1.1$lendbase$Pearson,
              base.model1.2$lendbase$Pearson,
              base.model1.3$lendbase$Pearson,
              base.model1.4$lendbase$Pearson),
  Scenario = factor(c(rep("s1.1", length(base.model1.1$lendbase$Pearson)),
                      rep("s1.2", length(base.model1.2$lendbase$Pearson)),
                      rep("s1.3", length(base.model1.3$lendbase$Pearson)),
                      rep("s1.4", length(base.model1.4$lendbase$Pearson))))
)


pearson_plot <- ggplot(dfpearson_long, aes(x = Scenario, 
                                            y = Pearson, 
                                            fill = Scenario)) +
  geom_boxplot(width = 0.2, alpha = 0.8,
               outliers = FALSE) +
  #geom_jitter(width = 0.01, alpha = 0.3) +
  theme_minimal() +
  scale_fill_manual(name = "Scenario",
                    values = c("s1.1" = "#ca0020", 
                               "s1.2" = "#f4a582", 
                               "s1.3" = "#bababa", 
                               "s1.4" = "#404040")) +
  labs(title = "", x = "", y = "Residual") +
  theme(legend.position = "none")

```


### Comparision RMSE

```{r fig.height=3, fig.width=3}
dfrmse <- data.frame(
  s1.1 = as.numeric(base.model1.1$index_variance_tuning_check$RMSE),
  s1.2 = as.numeric(base.model1.2$index_variance_tuning_check$RMSE[1:10]),
  s1.3 = as.numeric(base.model1.3$index_variance_tuning_check$RMSE[1:10]),
  s1.4 = as.numeric(base.model1.4$index_variance_tuning_check$RMSE[1:10])
  
)
summary(dfrmse)

t.test(dfrmse$s1.1, dfrmse$s1.2)  # Comparar s1.1 vs s1.2
t.test(dfrmse$s1.1, dfrmse$s1.3)  # Comparar s1.1 vs s1.3
t.test(dfrmse$s1.2, dfrmse$s1.3)
t.test(dfrmse$s1.3, dfrmse$s1.4)
anova_rmse <- aov(as.numeric(unlist(dfrmse)) ~ rep(1:4, each=nrow(dfrmse)))

dfrmse_long <- reshape2::melt(dfrmse)
rmse <-ggplot(dfrmse_long, aes(x = variable, 
                        y = value, 
                        fill= variable)) +
  geom_boxplot(width=0.2,
               alpha=0.8,
               outliers = FALSE) +
  #geom_jitter(width = 0.01, alpha = 0.3) +
  theme_minimal() +
  scale_fill_manual(name= "Scenario",
                    values = c("s1.1" = "#ca0020", 
                               "s1.2" = "#f4a582", 
                               "s1.3" = "#bababa", 
                               "s1.4" = "#404040")) +
  labs(title = "", x = "", y = "RMSE")+
  theme(legend.position = "none")
```

```{r}
ggarrange(rmse,
          pearson_plot,
          ncol=2)
```


This boxplot compares a summary of the Root Mean Square Error (RMSE) across four different models (s1.1, s1.2, s1.3, and s1.4) used to evaluate recruitment estimates of Antarctic krill. RMSE serves as an indicator of model accuracy, with lower values representing better predictive performance.

s1.1 exhibits the lowest median RMSE, suggesting it has the best overall fit among the four models. In contrast, Models 1.2, 1.3, and 1.4 show higher RMSE values, indicating comparatively lower predictive accuracy. The interquartile range (IQR) of these models is relatively similar, suggesting comparable variability in RMSE across models. Additionally, s1.1 has an outlier above 1.5 RMSE, which could indicate a case where the model's predictions deviated significantly from observed values.

Overall, this analysis highlights that incorporating different environmental or predator-related variables in the models impacts their predictive ability. The differences in RMSE suggest that some models may overfit or underfit the recruitment patterns of krill, emphasizing the need to refine model selection based on ecological and statistical considerations.


### Relationship Stock-Recruit

\[
R = \frac{aS}{1 + bS}
\]

Where:  
- \( S \) is the spawning stock biomass.  
- \( R \) is the predicted recruitment.  
- \( a \) is the maximum recruitment capacity.  
- \( b \) regulates density dependence (higher values of \( b \) result in a lower plateau).  

The blue line will show the asymptotic curve that describes the relationship between spawning stock biomass and recruitment.


```{r}
# Lista de modelos base
models <- list(
  s1.1 = base.model1.1,
  s1.2 = base.model1.2,
  s1.3 = base.model1.3,
  s1.4 = base.model1.4
)

colors <- c("s1.1" = "#ca0020", "s1.2" = "#f4a582", "s1.3" = "#bababa", "s1.4" = "#404040")
shapes <- c("s1.1" = 5, "s1.2" = 6, "s1.3" = 15, "s1.4" = 4)

plot_data <- list()
curve_data <- list()
label_data <- list()
params <- data.frame(Model = character(), a = numeric(), b = numeric(), stringsAsFactors = FALSE)
for (group in names(models)) {
  model <- models[[group]]
  l <- model$recruit$SpawnBio
  m <- model$recruit$pred_recr
  yr <- model$timeseries$Yr[1:35]
  
  # Filtrar valores válidos
  valid <- which(is.finite(l) & is.finite(m) & l > 0 & m > 0)
  l <- l[valid]
  m <- m[valid]
  yr <- yr[valid]
  
  plot_data[[group]] <- data.frame(l = l, m = m, yr = yr, Group = group)
  label_data[[group]] <- plot_data[[group]] %>% filter(yr %% 3 == 0)
  
  fit <- NULL
  tryCatch({
    fit <- nls(m ~ (a * l) / (1 + b * l),
               start = list(a = max(m), b = 1 / max(l)),
               data = data.frame(l, m))
    
    l_pred <- seq(min(l), max(l), length.out = 100)
    m_pred <- predict(fit, newdata = data.frame(l = l_pred))
    curve_data[[group]] <- data.frame(l = l_pred, m = m_pred, Group = group)
    
    coefs <- coef(fit)
    params <- rbind(params, data.frame(Model = group, a = coefs["a"], b = coefs["b"]))
    
  }, error = function(e) {
    message(paste("Model", group, ": Beverton-Holt fit failed ->", e$message))
  })
}

# Unir data frames
plot_data_all <- bind_rows(plot_data)
curve_data_all <- bind_rows(curve_data)
label_data_all <- bind_rows(label_data)

# Graficar
srr_sce <- ggplot() +
  geom_point(data = plot_data_all, aes(x = l, y = m, shape = Group), size = 2) +
  geom_line(data = curve_data_all, aes(x = l, y = m, color = Group), linewidth = 1.2) +
  ggrepel::geom_text_repel(data = label_data_all, aes(x = l, y = m, label = yr, color = Group),
                           size = 4, max.overlaps = 50) +
  scale_shape_manual(values = shapes) +
  scale_color_manual(values = colors) +
  theme_bw() +
  ylim(0, max(plot_data_all$m)) +
  labs(x = "Spawning Stock Biomass (SSB)", y = "Predicted Recruitment", shape = "Model", color = "Model") +
  theme(legend.position = "bottom")
```


### Productivity and Interannual Variability by Scenario

Estimating the productivity of Antarctic krill is critical for understanding the species’ capacity to replenish its population in response to varying levels of spawning biomass. Productivity, defined as the ratio of recruitment to spawning stock biomass, provides a standardized measure of reproductive success and population resilience under different ecological and fishing pressures. Comparing productivity across scenarios—each representing different assumptions about environmental drivers, fishing mortality, or predator dynamics—enables a robust evaluation of how krill populations reflect this changes.

For each scenario \( i \) and year \( t \), we computed the productivity as the ratio between recruitment and spawning stock biomass (SSB):

$$
\text{Productivity}_{i,t} = \frac{\text{Recruitment}_{i,t}}{\text{SSB}_{i,t}}
$$

We also calculated the **interannual percentage change** in recruitment and SSB as:

$$
\text{Change in Recruitment}_{i,t} = \left( \frac{\text{Recruitment}_{i,t} - \text{Recruitment}_{i,t-1}}{\text{Recruitment}_{i,t-1}} \right) \times 100
$$

$$
\text{Change in SSB}_{i,t} = \left( \frac{\text{SSB}_{i,t} - \text{SSB}_{i,t-1}}{\text{SSB}_{i,t-1}} \right) \times 100
$$

These metrics allow us to analyze both the productivity and the temporal dynamics of the population under each scenario \( i \).


```{r}
# Simular 4 dataframes por escenario (usa tus datos reales)
escenarios <- list(
  s1.1 = base.model1.1$SPAWN_RECR_CURVE,
  s1.2 = base.model1.2$SPAWN_RECR_CURVE,
  s1.3 = base.model1.3$SPAWN_RECR_CURVE,
  s1.4 = base.model1.4$SPAWN_RECR_CURVE
)

# Crea una tabla con productividad y cambios para cada escenario
resultados <- purrr::imap_dfr(escenarios, function(df, nombre) {
  df %>%
    mutate(
      Scenario = nombre,
      Productividad = Recruitment / SSB,
      Cambio_Recruit = c(NA, diff(Recruitment)) / lag(Recruitment) * 100,
      Cambio_SSB = c(NA, diff(SSB)) / lag(SSB) * 100
    )
})


prod <- ggplot(resultados, aes(x = `SSB/SSB_virgin`, y = Productividad, 
                       color = Scenario)) +
  geom_point(size = 1.1) +
  labs(title = "",
       y = "Recruitment / SSB",
       x = "SSB / SSB_virgin") +
  scale_color_manual(name= "",
                    values = c("s1.1" = "#ca0020", 
                               "s1.2" = "#f4a582", 
                               "s1.3" = "#bababa", 
                               "s1.4" = "#404040")) +
  theme_bw() +
  theme(legend.position = "bottom")
```


These two panels provide insight into the stock-recruitment dynamics of Antarctic krill under four assessment scenarios, each incorporating different levels of ecosystem complexity.

**Left panel – Predicted Recruitment vs Spawning Stock Biomass (SSB):**  
This plot compares the predicted recruitment across years with corresponding spawning biomass values for the four model scenarios (s1.1 to s1.4). Scenario s1.1 (red) shows a relatively flat and optimistic Beverton-Holt relationship, with high recruitment predicted even at low levels of SSB. The other scenarios, particularly s1.3 (gray) and s1.4 (black), show much more constrained recruitment estimates and a more saturating relationship, where recruitment increases only slightly with increasing SSB. This suggests that incorporating environmental (s1.3) and predator (s1.4) effects leads to a more conservative and ecologically realistic recruitment dynamic. Year labels illustrate how certain years (e.g., 2004, 2016, 2022) deviate between scenarios, especially at low biomass levels.

**Right panel – Recruitment per SSB vs Relative SSB (SSB / SSBo):**  
This plot displays recruitment efficiency (Recruitment/SSB) as a function of relative spawning biomass (SSB / SSB₀). In all models, recruitment efficiency is highest at very low biomass levels and declines as SSB increases, consistent with compensatory dynamics. However, s1.3 and s1.4 (which include environmental effects) show significantly lower overall productivity across all SSB levels compared to s1.1 and s1.2. This indicates that the inclusion of ecosystem drivers reduces the per-capita productivity of the stock, especially under depleted conditions.

**Interpretation:**  
Together, these plots indicate that ecosystem-informed models (s1.3 and s1.4) produce more constrained and precautionary estimates of krill productivity. The more optimistic recruitment predicted by s1.1 (and to a lesser extent s1.2) may overestimate stock resilience, particularly in low SSB conditions. These findings emphasize the importance of accounting for environmental variability and predator dynamics in stock assessment models to avoid overoptimistic management advice and better reflect the biological limits of the krill population.

```{r, fig.height=4, fig.width=6, fig.cap="Left panel: Stock–recruitment relationships for Antarctic krill under four assessment scenarios. Predicted recruitment as a function of spawning stock biomass (SSB), with fitted Beverton-Holt curves and annual labels for selected years. Right panel: Recruitment per unit of SSB plotted against relative SSB (SSB/SSBo), illustrating differences in per-capita productivity across models ( s1.1 (no ecosystem variables), s1.2 (with predator effects), s1.3 (with environmental variables), and s1.4 (with both predator and environmental effects))"}
ggarrange(srr_sce,
          prod,
          ncol = 2)
```

### Compara Fishing Mortality.

```{r eval=FALSE}
escenarios <- list(
  s1.1 = base.model1.1$exploitation[1:5],
  s1.2 = base.model1.2$exploitation[1:5],
  s1.3 = base.model1.3$exploitation[1:5],
  s1.4 = base.model1.4$exploitation[1:5]
)

# Calcula el cambio porcentual entre años para F_std y annual_F
# Calcula el cambio relativo entre años para F_std y annual_F
# Calcula el cambio relativo entre años para F_std y annual_F
resultados <- purrr::imap_dfr(escenarios, function(df, nombre) {
  df %>%
    mutate(
      Scenario = nombre,
      Relativo_Fstd = c(NA, diff(F_std) / head(F_std, -1)),  # Cambio relativo de F_std
      Relativo_annual_F = c(NA, diff(annual_F) / head(annual_F, -1))  # Cambio relativo de annual_F
    )
})


# Visualiza los resultados en un gráfico
ggplot(resultados, aes(x = Yr, y = Relativo_Fstd, color = Scenario)) +
  geom_line(size = 1.1) +
  labs(title = "Cambio porcentual en F_std entre años por escenario",
       x = "Año",
       y = "Cambio porcentual en F_std") +
  scale_color_manual(name= "Scenario",
                    values = c("s1.1" = "#ca0020", 
                               "s1.2" = "#f4a582", 
                               "s1.3" = "#bababa", 
                               "s1.4" = "#404040")) +
  theme_minimal(base_size = 13) +
  theme(legend.position = "bottom")

# Gráfico para annual_F
ggplot(resultados, aes(x = Yr, y = Relativo_annual_F, color = Scenario)) +
  geom_line(size = 1.1) +
  labs(title = "Cambio porcentual en annual_F entre años por escenario",
       x = "Año",
       y = "Cambio porcentual en annual_F") +
  scale_color_manual(name= "Scenario",
                    values = c("s1.1" = "#ca0020", 
                               "s1.2" = "#f4a582", 
                               "s1.3" = "#bababa", 
                               "s1.4" = "#404040")) +
  theme_minimal(base_size = 13) +
  theme(legend.position = "bottom")

```

### Retrospective Analysis in Model Evaluation

Retrospective analyses provide insights into the differences in estimation patterns (underestimation or overestimation) among the models evaluated. These analyses assess the consistency and reliability of stock assessment models by systematically removing the most recent years of data and comparing the resulting estimates with the full dataset.  

In this study, we conducted a retrospective analysis to examine the sensitivity of our recruitment and spawning stock biomass (SSB) estimates to the inclusion or exclusion of recent data. By applying this approach to multiple models, we identified potential biases and evaluated the stability of the recruitment estimates over time.  

The retrospective patterns were assessed by calculating the relative error between the predictions of truncated datasets and the full dataset. These differences allowed us to detect trends in model performance, such as systematic overestimation or underestimation of key population parameters. Understanding these deviations is crucial for improving the robustness of the stock assessment models and ensuring more reliable projections for fisheries management.

```{r eval=FALSE}
#one by one
retro(
    dir = dir1.1,
    oldsubdir = "",
    newsubdir = "Retrospective",
    years = 0:-4,
    exe = "ss_osx",
    extras = "-nox",
    skipfinished = FALSE)
```


```{r eval=FALSE}
directorios <- c("s1.1",
                 "s1.2",
                 "s1.3",
                 "s1.4")  
for (dir in directorios) {
  retro(
    dir = dir,
    oldsubdir = "",
    newsubdir = "Retrospective",
    years = 0:-5,
    exe = "ss_osx",
    extras = "-nox",
    skipfinished = FALSE
  )
}
```

```{r message=FALSE, warning=FALSE}
#stest
retroModels1.1 <- SSgetoutput(dirvec=file.path(dir1.1,
                                            "Retrospective",
                                            paste("retro",0:-4,
                                                  sep="")))

retroSummary1.1 <- SSsummarize(retroModels1.1)
#stest
retroModels1.2 <- SSgetoutput(dirvec=file.path(dir1.2,
                                            "Retrospective",
                                            paste("retro",0:-4,
                                                  sep="")))

retroSummary1.2 <- SSsummarize(retroModels1.2)
#stest
retroModels1.3 <- SSgetoutput(dirvec=file.path(dir1.3,
                                            "Retrospective",
                                            paste("retro",0:-4,
                                                  sep="")))

retroSummary1.3 <- SSsummarize(retroModels1.3)
#stest
retroModels1.4 <- SSgetoutput(dirvec=file.path(dir1.4,
                                            "Retrospective",
                                            paste("retro",0:-4,
                                                  sep="")))

retroSummary1.4 <- SSsummarize(retroModels1.4)


```

```{r}
par(mfrow=c(2,2), mar=c(5,4,1,1)) 
retro1.1 <- SSplotRetro(retroSummary1.1,
            add=T,
            forecast = F,
            legend = T,
            verbose=F,
            subplots = c("SSB"))
retro1.2 <- SSplotRetro(retroSummary1.2,
            add=T,
            forecast = F,
            legend = T,
            verbose=F,
            subplots = c("SSB"))
retro1.3 <- SSplotRetro(retroSummary1.3,
            add=T,
            forecast = F,
            legend = T,
            verbose=F,
            subplots = c("SSB"))
retro1.4 <- SSplotRetro(retroSummary1.4,
            add=T,
            forecast = F,
            legend = T,
            verbose=F,
            subplots = c("SSB"))
```

```{r}
par(mfrow=c(2,2), mar=c(5,4,1,1)) 
retro1.1 <- SSplotRetro(retroSummary1.1,
            add=T,
            forecast = F,
            legend = T,
            verbose=F,
            subplots = c("F"))
retro1.2 <- SSplotRetro(retroSummary1.2,
            add=T,
            forecast = F,
            legend = T,
            verbose=F,
            subplots = c("F"))
retro1.3 <- SSplotRetro(retroSummary1.3,
            add=T,
            forecast = F,
            legend = T,
            verbose=F,
            subplots = c("F"))
retro1.4 <- SSplotRetro(retroSummary1.4,
            add=T,
            forecast = F,
            legend = T,
            verbose=F,
            subplots = c("F"))
```

### Hindcast Cross-Validation and Prediction Skill

The Hindcast Cross-Validation (HCxval) diagnostic in Stock Synthesis is implemented using the model outputs generated by the `r4ss::SS_doRetro()` function. This diagnostic evaluates the predictive performance of the model by comparing hindcast predictions with observed data.

To assess prediction skill, we employ the Mean Absolute Scaled Error (MASE) as a robust metric. MASE is calculated by scaling the mean absolute error of the model predictions relative to the mean absolute error of a naïve baseline prediction. Specifically, the MASE score is computed as follows:

- A MASE score greater than 1 indicates that the model’s average forecasts are less accurate than a random walk model.
- A MASE score equal to 1 suggests that the model’s accuracy is similar to that of a random walk.
- A MASE score less than 1 indicates that the model performs better than a random walk.
- A MASE score of 0.5, for example, indicates that the model’s forecasts are twice as accurate as the naïve baseline prediction, suggesting the model has predictive skill.

This approach provides a rigorous evaluation of model forecasting capabilities and helps identify improvements for model calibration.


```{r}
par(mfrow=c(2,4), mar=c(5,4,2,1))
hci1 = SSplotHCxval(retroSummary1.1, 
                   add = T, 
                   verbose = F, 
                   legendcex = 0.7)
```


```{r}
par(mfrow=c(3,3), mar=c(5,4,2,1))
hci2 = SSplotHCxval(retroSummary1.2, 
                   add = T, 
                   verbose = F, 
                   legendcex = 0.7)
```


```{r}
par(mfrow=c(2,4), mar=c(5,4,2,1))
hci3 = SSplotHCxval(retroSummary1.3, 
                   add = T, 
                   verbose = F, 
                   legendcex = 0.7)
```


```{r}
par(mfrow=c(3,3), mar=c(5,4,2,1))
hci4 = SSplotHCxval(retroSummary1.4, 
                   add = T, 
                   verbose = F, 
                   legendcex = 0.7) 
```

### Kobe (status)


```{r eval=FALSE}
mvln = SSdeltaMVLN(base.model1.1,
                   Fref = "MSY", 
                   plot = TRUE,
                   addprj=TRUE,
                   virgin = TRUE,
                   mc=100)
mvln = SSdeltaMVLN(base.model1.2,
                   Fref = "MSY", 
                   plot = TRUE,
                   addprj=TRUE,
                   virgin = TRUE,
                   mc=100)
mvln = SSdeltaMVLN(base.model1.3,
                   Fref = "MSY", 
                   plot = TRUE,
                   addprj=TRUE,
                   virgin = TRUE,
                   mc=100)
mvln = SSdeltaMVLN(base.model1.4,
                   Fref = "MSY", 
                   plot = TRUE,
                   addprj=TRUE,
                   virgin = TRUE,
                   mc=100)

```
another

```{r eval=FALSE}
nanios<-base.model1.1$Kobe$Yr
SSBDep<-base.model1.1$Kobe$B.Bmsy
y_BDPR<-base.model1.1$Kobe$F.Fmsy

kobe1 <- ggplot(base.model1.1$Kobe)+
  geom_rect(aes(xmin = 0, xmax = 0.5, ymin = 0, ymax = 5),
            fill = "#E43338", alpha = 0.5) +
  geom_rect(aes(xmin = 0.5, xmax = 0.75, ymin = 0, ymax = 5),
            fill = "#F2ED23", alpha = 0.5) +
  geom_rect(aes(xmin = 0.75, xmax = 1.25, ymin = 0, ymax = 5),
            fill = "#ACC39A", alpha = 0.5) +
  geom_rect(aes(xmin = 1.25, xmax = 9, ymin = 0, ymax = 1),
            fill = "#608D68", alpha = 0.5) +
  geom_rect(aes(xmin = 1.25, xmax = 9, ymin = 1, ymax = 5),
            fill = "#808080", alpha = 0.5) +
  geom_path(aes(x=base.model1.1$Kobe$B.Bmsy,y=base.model1.1$Kobe$F.Fmsy,
                label=base.model1.1$Kobe$Yr))+
  geom_point(aes(base.model1.1$Kobe$B.Bmsy, base.model1.1$Kobe$F.Fmsy),
             lwd=2) +
  geom_hline(yintercept = 1) +
  geom_vline(xintercept = c(0.5, 0.75, 1.75, 1, 1.25), linetype=2)+
  theme_few()+
  labs(x = expression("BD/BD"[RMS]), y = expression("F/F"[RMS]))+
  # geom_text(data = texto_coords, aes(x = x, y = y, label = etiqueta),
  #            vjust = -0.5)+
  geom_text(aes(x=base.model1.1$Kobe$B.Bmsy,y=base.model1.1$Kobe$F.Fmsy,label=base.model1.1$Kobe$Yr),
            nudge_y = 0.1,size = 3,
            check_overlap = TRUE)
kobe1
```

```{r}
# model01
tablebias01 <- SShcbias(retroSummary1.1,quant="SSB",verbose=F)
tablebias01a <- SShcbias(retroSummary1.1,quant="F",verbose=F)

kbl(tablebias01, booktabs = T,format = "latex",
    caption = "Rho parameter in SSB model s01")  %>% 
    kable_styling(latex_options = "HOLD_position")

kbl(tablebias01a, booktabs = T,format = "latex",
    caption = "Rho parameter in F model s01")  %>% 
    kable_styling(latex_options = "HOLD_position")

# model2
tablebias2 <- SShcbias(retroSummary1.2,quant="SSB",verbose=F)
tablebias2a <- SShcbias(retroSummary1.2,quant="F",verbose=F)

kbl(tablebias2, booktabs = T,format = "latex",
    caption = "Rho parameter in SSB model s2")  %>% 
    kable_styling(latex_options = "HOLD_position")

kbl(tablebias2a, booktabs = T,format = "latex",
    caption = "Rho parameter in F model s2")  %>% 
    kable_styling(latex_options = "HOLD_position")

# model3
tablebias3 <- SShcbias(retroSummary1.3,quant="SSB",verbose=F)
tablebias3a <- SShcbias(retroSummary1.3,quant="F",verbose=F)

kbl(tablebias3, booktabs = T,format = "latex",
    caption = "Rho parameter in SSB model s3")  %>% 
    kable_styling(latex_options = "HOLD_position")

kbl(tablebias3a, booktabs = T,format = "latex",
    caption = "Rho parameter in F model s3")  %>% 
    kable_styling(latex_options = "HOLD_position")


# model4
tablebias4 <- SShcbias(retroSummary1.4,quant="SSB",verbose=F)
tablebias4a <- SShcbias(retroSummary1.4,quant="F",verbose=F)

kbl(tablebias4, booktabs = T,format = "latex",
    caption = "Rho parameter in SSB model s4")  %>% 
    kable_styling(latex_options = "HOLD_position")

kbl(tablebias4a, booktabs = T,format = "latex",
    caption = "Rho parameter in F model s4")  %>% 
    kable_styling(latex_options = "HOLD_position")


# Combine the results from all models into one table
tablebias_combined <- bind_rows(
  data.frame(Model = "s1.1", 
             Quant = "SSB", 
             Rho = SShcbias(retroSummary1.1, 
                            quant = "SSB", verbose = F)),
  data.frame(Model = "s1.1", 
             Quant = "F", 
             Rho = SShcbias(retroSummary1.1, 
                            quant = "F", verbose = F)),
  data.frame(Model = "s1.2", 
             Quant = "SSB", 
             Rho = SShcbias(retroSummary1.2, 
                            quant = "SSB", verbose = F)),
  data.frame(Model = "s1.2", 
             Quant = "F", 
             Rho = SShcbias(retroSummary1.2, 
                            quant = "F", verbose = F)),
  data.frame(Model = "s1.3", 
             Quant = "SSB", 
             Rho = SShcbias(retroSummary1.3, 
                            quant = "SSB", verbose = F)),
  data.frame(Model = "s1.3", 
             Quant = "F", 
             Rho = SShcbias(retroSummary1.3, 
                            quant = "F", verbose = F)),
  data.frame(Model = "s1.4", 
             Quant = "SSB", 
             Rho = SShcbias(retroSummary1.4, 
                            quant = "SSB", verbose = F)),
  data.frame(Model = "s1.4",
             Quant = "F", 
             Rho = SShcbias(retroSummary1.4, 
                            quant = "F", verbose = F))
)
```
Viz

```{r, fig.height=4, fig.width=6}
tablebias_combined <- tablebias_combined %>%
  mutate(Rho.peel = factor(Rho.peel, levels = unique(Rho.peel)))

ggplot(tablebias_combined, aes(x = Rho.peel, y = Rho.Rho, group = Model, fill = Model)) +
  geom_point(size = 3, shape = 21, color = "black") +  
  geom_hline(yintercept = 0, color = "grey") +
  facet_wrap(~Quant, scales = "free_y") + 
  scale_fill_manual(name = "Scenario",
                    values = c("s1.1" = "#ca0020", 
                               "s1.2" = "#f4a582", 
                               "s1.3" = "#bababa", 
                               "s1.4" = "#404040")) +
  scale_color_manual(name = "Scenario",
                     values = c("s1.1" = "#ca0020", 
                                "s1.2" = "#f4a582", 
                                "s1.3" = "#bababa", 
                                "s1.4" = "#404040")) +
  theme_minimal() +
  labs(title = "",
       x = "",
       y = "Rho Value") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        legend.position = "bottom")


```
 
The plot presents the retrospective bias (*Rho*) in Spawning Stock Biomass (SSB) and Fishing Mortality (F) across different peel years (2019–2016) and for the combined period under four model scenarios (s1.1 to s1.4). In the SSB panel, all models exhibit negative *Rho* values across the years, indicating a downward bias. The magnitude of the bias varies among models, with s1.3 and s1.4 showing the most pronounced deviations. In the F panel, *Rho* values are consistently positive, suggesting an upward bias. The combined *Rho* values for each metric confirm these trends, showing persistent differences among model scenarios.  

(Discussion)

The observed retrospective bias in SSB and F aligns with findings from previous studies on krill population dynamics, where recruitment variability and model structure influence stock assessment estimates (Hill et al., 2016; Watters et al., 2020). The negative bias in SSB suggests that models may overestimate past biomass, potentially due to unaccounted environmental variability or underestimation of natural mortality (Meyer et al., 2021). The positive bias in F indicates a tendency to underestimate past fishing mortality, which could be linked to the inclusion of predator-based indices such as penguin foraging data (Trathan et al., 2018). These results highlight the importance of incorporating ecosystem interactions into stock assessments to improve predictive accuracy and support sustainable management of Antarctic krill.


```{r}
kbl(tablebias_combined, booktabs = T, 
    format = "html", 
    caption = "Rho parameter by model and quantity (SSB and F)") %>% 
  kable_styling(latex_options = "HOLD_position")


```

### Likelihood tables

```{r}
like1 <- base.model1.1$likelihoods_used
like1$model <- rep("s1.1", nrow(like1))
like1 <- rownames_to_column(like1, var = "Description")
like2 <- base.model1.2$likelihoods_used
like2$model <- rep("s1.2", nrow(like2))
like2 <- rownames_to_column(like2, var = "Description")
like3 <- base.model1.3$likelihoods_used
like3$model <- rep("s1.3", nrow(like3))
like3 <- rownames_to_column(like3, var = "Description")
like4 <- base.model1.4$likelihoods_used
like4$model <- rep("s1.4", nrow(like4))
like4 <- rownames_to_column(like4, var = "Description")

totalike <- rbind(like1,
                   like2,
                   like3,
                   like4)
```

```{r}
ggplot(totalike %>% 
         filter(values>1))+
  geom_bar( aes(y = reorder(Description, values), 
                x =log(values)),
            stat = "identity", position = "dodge") +
  theme_few() +
  facet_wrap(.~model, 
             ncol=4)+
  labs(y="",
       x="Log Likelihood")+
  theme(axis.text.x = element_text(angle = 90, 
                                   hjust = 1))+
  xlim(0,30)
```

### Likelihood Profile 

```{r eval=FALSE, echo = FALSE}
# 1. Identificar el directorio donde se encuentra el modelo base ----
dirname.model.run <- here("s1.2")
# 2. Crear un nuevo directorio para el "Perfil_Verosimilitud"  
dirname.R0.profile <- here("s1.2",
                           "Perfil_Verosimilitud")
dir.create(path=dirname.R0.profile, 
           showWarnings = TRUE, 
           recursive = TRUE)
# 3. Crear un subdirectorio llamado "plots_Verosimilitud" ----
plotdir=paste0(dirname.R0.profile, "/plots_Verosimilitud")
dir.create(path=plotdir,
           showWarnings = TRUE, 
           recursive = TRUE)
# 4. Crear un subdirectorio llamado "simple" ----
reference.dir <- paste0(dirname.R0.profile,'/simple') 
dir.create(path=reference.dir, showWarnings = TRUE, recursive = TRUE)
# 5. Copiar el resultado del modelo base completo en este directorio ----
file.copy(Sys.glob(paste(dirname.model.run, "*.*", sep="/"),
                   dirmark = FALSE),
                    reference.dir)
# 6. Leer la salida del modelo base ----
Base <- SS_output(dir=reference.dir,covar=T)
# 7. Copiar los archivos necesarios de "simple" al directorio "Perfil_Verosimilitud" ----
copy_SS_inputs(dir.old = reference.dir, 
               dir.new =  dirname.R0.profile,
               copy_exe = TRUE,
               verbose = FALSE)
# 8. Leer los archivos del modelo ----
inputs <- r4ss::SS_read(dir = dirname.R0.profile)
# 9. Editar el archivo control la fase de estimación recdev ----
inputs$ctl$recdev_phase <- 1
# 10. Editar el archivo starter para leer los valores de inicio ----
inputs$start$init_values_src <- 0
# 11. Vector de valores para el perfil ----
R0.vec <- seq(18,30,1)  
Nprof.R0 <- length(R0.vec)
# 12. Cambiar el nombre del archivo control en el archivo starter.ss ----
inputs$start$ctlfile <- "control_modified.ss" 
# 13. Incluir prior_like para parámetros no estimados ----
inputs$start$prior_like <- 1                                 
# 14. Escribir los modelos modificados ----
r4ss::SS_write(inputs, dir = dirname.R0.profile, overwrite = TRUE)
# 15. Ejecutar la función profile() ----
#?SS_profile()
profile <- profile(dir=dirname.R0.profile, # directory
                      exe="ss_osx",
                      oldctlfile ="control.ss",
                      newctlfile="control_modified.ss",
                      string="SR_LN(R0)",
                      profilevec=R0.vec)
# 16. Leer los archivos de salida ----
# (con nombres como Report1.sso, Report2.sso, etc.)
prof.R0.models <- SSgetoutput(dirvec=dirname.R0.profile, 
                              keyvec=1:Nprof.R0, 
                              getcovar = FALSE) 
# 17. Resumir las salidas con la función SSsummarize()  ----
prof.R0.summary <- SSsummarize(prof.R0.models)
# 18. Identificar los componentes de Verosimilitud ----
mainlike_components         <- c('TOTAL',
                                 "Survey", 
                                 'Length_comp',
                                 "Age_comp",
                                 "Catch",
                                 'Size_at_age',
                                 'Recruitment') 
mainlike_components_labels  <- c('Total likelihood',
                                 'Index likelihood',
                                 'Length likelihood',
                                 "Age likelihood",
                                 "Catch Likelihood",
                                 'Size_at_age likelihood',
                                 'Recruitment likelihood') 

```


### Likelihood Profile

```{r eval=FALSE, echo = FALSE}
png(file.path(plotdir,"R0_profile_plot.png"),
    width=7,
    height=4.5,
    res=300,
    units='in')
par(mar=c(5,4,1,1))
SSplotProfile(prof.R0.summary,           # summary object
              profile.string = "R0",     # substring of profile parameter
              profile.label=expression(log(italic(R)[0])), 
              ymax=2050,minfraction = 0.001,
              pheight=4.5, 
              print=FALSE, 
              plotdir=plotdir, 
              components = mainlike_components, 
              component.labels = mainlike_components_labels,
              add_cutoff = TRUE,
              cutoff_prob = 0.95)

Baseval <- round(Base$parameters$Value[grep("R0",Base$parameters$Label)],2)
Baselab <- paste(Baseval,sep="")
axis(1,at=Baseval,label=Baselab)
abline(v = Baseval, lty=2)
dev.off()
```






```{r eval=FALSE, echo = FALSE}
# Comparación de series de tiempo 
labs <- paste("SR_Ln(R0) = ",R0.vec)
labs[which(round(R0.vec,2)==Baseval)] <- paste("SR_Ln(R0) = ",
                                               Baseval,"(Base model)")

SSplotComparisons(prof.R0.summary,
                  legendlabels=labs,
                  pheight=4.5,png=TRUE,
                  plotdir=plotdir,
                  legendloc='bottomleft')


```

piner Plot

```{r eval = FALSE}
#### R0_profile_plot_Length_like ----
png(file.path(plotdir,"R0_profile_plot_Length_like.png"),
    width=7,
    height=4.5,
    res=300,
    units='in')
par(mar=c(5,4,1,1))
PinerPlot(prof.R0.summary, 
          profile.string = "R0", 
          component = "Length_like",
          main = "Changes in length-composition likelihoods by fleet",
          add_cutoff = TRUE,
          cutoff_prob = 0.95)
Baseval <- round(Base$parameters$Value[grep("SR_LN",
                                      Base$parameters$Label)],2)
Baselab <- paste(Baseval,sep="")
axis(1,at=Baseval,
     label=Baselab)
abline(v = Baseval, lty=2)
dev.off()
```


```{r eval=FALSE}
#### R0_profile_plot_Survey_like ----
png(file.path(plotdir,"R0_profile_plot_Survey_like.png"),
    width=7,
    height=4.5,
    res=300,
    units='in')
par(mar=c(5,4,1,1))
PinerPlot(prof.R0.summary, 
          profile.string = "R0", 
          component = "Surv_like",
          main = "Changes in Index likelihoods by fleet",
          add_cutoff = TRUE,
          cutoff_prob = 0.95, legendloc="topleft")
Baseval <- round(Base$parameters$Value[grep("SR_LN",
                                            Base$parameters$Label)],2)
Baselab <- paste(Baseval,sep="")
axis(1,at=Baseval,label=Baselab)
abline(v = Baseval, lty=2)
dev.off()
```

### Table Diagnosis



```{r}
# Modelo s1.1
diag1.1 <- data.frame(
  Scenario = "s1.1",
  Convergency = base.model1.1$maximum_gradient_component,
  AIC = as.numeric(2 * dim(base.model1.1$estimated_non_dev_parameters)[1] + 2 * base.model1.1$likelihoods_used[1, 1]),
  Total_like = base.model1.1$likelihoods_used$values[rownames(base.model1.1$likelihoods_used) == "TOTAL"],
  N_Params = as.numeric(2 * dim(base.model1.1$estimated_non_dev_parameters)[1]),
  Survey_like = base.model1.1$likelihoods_used$values[rownames(base.model1.1$likelihoods_used) == "Survey"],
  Length_comp_like = base.model1.1$likelihoods_used$values[rownames(base.model1.1$likelihoods_used) == "Length_comp"],
  RMSE_index = rmse1c$RMSE.perc[rmse1c$indices == "Combined"],
  RMSE_length = rmse1l$RMSE.perc[rmse1l$indices == "Combined"],
  MASE = mean(hci1$MAE.base, na.rm = TRUE),
  Retro_Rho_ssb = tablebias01[5, 3],
  Forecast_Rho_ssb = tablebias01[5, 4],
  Forecast_Rho_f = tablebias01a[5, 3],
  Rho_f = tablebias01a[5, 4]
)

# Modelo s1.2
diag1.2 <- data.frame(
  Scenario = "s1.2",
  Convergency = base.model1.2$maximum_gradient_component,
  AIC = as.numeric(2 * dim(base.model1.2$estimated_non_dev_parameters)[1] + 2 * base.model1.2$likelihoods_used[1, 1]),
  Total_like = base.model1.2$likelihoods_used$values[rownames(base.model1.2$likelihoods_used) == "TOTAL"],
  N_Params = as.numeric(2 * dim(base.model1.2$estimated_non_dev_parameters)[1]),
  Survey_like = base.model1.2$likelihoods_used$values[rownames(base.model1.2$likelihoods_used) == "Survey"],
  Length_comp_like = base.model1.2$likelihoods_used$values[rownames(base.model1.2$likelihoods_used) == "Length_comp"],
  RMSE_index = rmse2c$RMSE.perc[rmse2c$indices == "Combined"],
  RMSE_length = rmse2l$RMSE.perc[rmse2l$indices == "Combined"],
  MASE = mean(hci2$MAE.base, na.rm = TRUE),
  Retro_Rho_ssb = tablebias2[5, 3],
  Forecast_Rho_ssb = tablebias2[5, 4],
  Forecast_Rho_f = tablebias2a[5, 3],
  Rho_f = tablebias2a[5, 4]
)

# Modelo s1.3
diag1.3 <- data.frame(
  Scenario = "s1.3",
  Convergency = base.model1.3$maximum_gradient_component,
  AIC = as.numeric(2 * dim(base.model1.3$estimated_non_dev_parameters)[1] + 2 * base.model1.3$likelihoods_used[1, 1]),
  Total_like = base.model1.3$likelihoods_used$values[rownames(base.model1.3$likelihoods_used) == "TOTAL"],
  N_Params = as.numeric(2 * dim(base.model1.3$estimated_non_dev_parameters)[1]),
  Survey_like = base.model1.3$likelihoods_used$values[rownames(base.model1.3$likelihoods_used) == "Survey"],
  Length_comp_like = base.model1.3$likelihoods_used$values[rownames(base.model1.3$likelihoods_used) == "Length_comp"],
  RMSE_index = rmse3c$RMSE.perc[rmse3c$indices == "Combined"],
  RMSE_length = rmse3l$RMSE.perc[rmse3l$indices == "Combined"],
  MASE = mean(hci3$MAE.base, na.rm = TRUE),
  Retro_Rho_ssb = tablebias3[5, 3],
  Forecast_Rho_ssb = tablebias3[5, 4],
  Forecast_Rho_f = tablebias3a[5, 3],
  Rho_f = tablebias3a[5, 4]
)

# Modelo s1.4
diag1.4 <- data.frame(
  Scenario = "s1.4",
  Convergency = base.model1.4$maximum_gradient_component,
  AIC = as.numeric(2 * dim(base.model1.4$estimated_non_dev_parameters)[1] + 
                     2 * base.model1.4$likelihoods_used[1, 1]),
  Total_like = base.model1.4$likelihoods_used$values[rownames(base.model1.4$likelihoods_used) == "TOTAL"],
  N_Params = as.numeric(2 * dim(base.model1.4$estimated_non_dev_parameters)[1]),
  Survey_like = base.model1.4$likelihoods_used$values[rownames(base.model1.4$likelihoods_used) == "Survey"],
  Length_comp_like = base.model1.4$likelihoods_used$values[rownames(base.model1.4$likelihoods_used) == "Length_comp"],
  RMSE_index = rmse4c$RMSE.perc[rmse4c$indices == "Combined"],
  RMSE_length = rmse4l$RMSE.perc[rmse4l$indices == "Combined"],
  MASE = mean(hci4$MAE.base, na.rm = TRUE),
  Retro_Rho_ssb = tablebias4[5, 3],
  Forecast_Rho_ssb = tablebias4[5, 4],
  Forecast_Rho_f = tablebias4a[5, 3],
  Rho_f = tablebias4a[5, 4]
)


diag_all <- rbind(diag1.1,
                  diag1.2, 
                  diag1.3, 
                  diag1.4)


diag_tidy <- diag_all |>
  pivot_longer(cols = -Scenario, names_to = "Description", values_to = "Value") |>
  pivot_wider(names_from = Scenario, values_from = Value)

diag_tidy[, 2:5] <- lapply(diag_tidy[, 2:5], function(x) as.numeric(format(round(x, 3), nsmall = 3)))


#write_csv(diag_tidy, "model_diagnosis.csv")
flextable(diag_tidy) |>
  colformat_num(digits = 3) |>  
  autofit() |>
  theme_booktabs() |>
  align(align = "center", part = "all") |>
  bold(part = "header")
```

## Outputs

### Outputs Model `s1.1`

```{r}
outps1 <- base.model1.1$timeseries[1:37, 2:8]

# addWorksheet(wb, "variable_s2")
# writeData(wb, "variable_s2", outps1)
# 
# # Guardar el workbook
# saveWorkbook(wb, "DataKrill.xlsx", overwrite = TRUE)
# 
# 
# out_s2 <- read_excel("DataKrill.xlsx", 
#     sheet = "variable_s2")
outps1 %>%
  kbl(booktabs = TRUE,
      format = "html",
    caption = "Main variables outputs from stock asssessment krill in WAP `s1.1`") %>%
  kable_paper("hover", 
              full_width = TRUE)%>%
  kable_styling(latex_options = c("striped",
                                  "condensed"),
                full_width = FALSE,
                font_size=9,
                html_font = "arial")#%>% 
  #pack_rows(index = c("Estimation" = 1,
   #                     "Prediction" = 45))
```
### Outputs Model `s1.2`

```{r}
outps2 <- base.model1.2$timeseries[1:37, 2:8]

# addWorksheet(wb, "variable_s2")
# writeData(wb, "variable_s2", outps1)
# 
# # Guardar el workbook
# saveWorkbook(wb, "DataKrill.xlsx", overwrite = TRUE)
# 
# 
# out_s2 <- read_excel("DataKrill.xlsx", 
#     sheet = "variable_s2")
outps2 %>%
  kbl(booktabs = TRUE,
      format = "html",
    caption = "Main variables outputs from stock asssessment krill in WAP in `s1.2`") %>%
  kable_paper("hover", 
              full_width = TRUE)%>%
  kable_styling(latex_options = c("striped",
                                  "condensed"),
                full_width = FALSE,
                font_size=9,
                html_font = "arial")#%>% 
  #pack_rows(index = c("Estimation" = 1,
   #                     "Prediction" = 45))
```



### Outputs Model `s1.3`

```{r}
outps3 <- base.model1.3$timeseries[1:37, 2:8]

# addWorksheet(wb, "variable_s2")
# writeData(wb, "variable_s2", outps1)
# 
# # Guardar el workbook
# saveWorkbook(wb, "DataKrill.xlsx", overwrite = TRUE)
# 
# 
# out_s2 <- read_excel("DataKrill.xlsx", 
#     sheet = "variable_s2")
outps3 %>%
  kbl(booktabs = TRUE,
      format = "html",
    caption = "Main variables outputs from stock asssessment krill in WAP in `s1.3`") %>%
  kable_paper("hover", 
              full_width = TRUE)%>%
  kable_styling(latex_options = c("striped",
                                  "condensed"),
                full_width = FALSE,
                font_size=9,
                html_font = "arial")#%>% 
  #pack_rows(index = c("Estimation" = 1,
   #                     "Prediction" = 45))
```

### Outputs Model `s1.4`

```{r}
outps4 <- base.model1.3$timeseries[1:37, 2:8]

# addWorksheet(wb, "variable_s2")
# writeData(wb, "variable_s2", outps1)
# 
# # Guardar el workbook
# saveWorkbook(wb, "DataKrill.xlsx", overwrite = TRUE)
# 
# 
# out_s2 <- read_excel("DataKrill.xlsx", 
#     sheet = "variable_s2")
outps4 %>%
  kbl(booktabs = TRUE,
      format = "html",
    caption = "Main variables outputs from stock asssessment krill in WAP in `s1.4`") %>%
  kable_paper("hover", 
              full_width = TRUE)%>%
  kable_styling(latex_options = c("striped",
                                  "condensed"),
                full_width = FALSE,
                font_size=9,
                html_font = "arial")#%>% 
  #pack_rows(index = c("Estimation" = 1,
   #                     "Prediction" = 45))
```



```{r}
# Unir los data frames en uno solo
outpsall <- rbind(outps1,  
                  outps2,
                  outps3,
                outps4)
```

```{r}
# Crear una columna de modelo en cada data frame
outps1$Model <- "s1.1 (Ref Model: No Env-Predator)"
outps2$Model <- "s1.2 w/ Predator data"
outps3$Model <- "s1.3 w/ Env data"
outps4$Model <- "s1.4 (Env and Predator data)"

# Unir los data frames en uno solo
outpsall <- rbind(outps1,  outps4)


outpsall_long <- outpsall %>%
  pivot_longer(cols = c(Bio_smry, SpawnBio, Recruit_0), 
               names_to = "Variable", values_to = "Value") %>% 
  filter(Yr >= 1989, Yr <= 2020)


# Gráfico de líneas por variable y modelo
alloutput <- ggplot(outpsall_long, aes(x = Yr, y = Value, color = Model, group = Model)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  facet_wrap(~Variable, 
             ncol = 1, 
             scales = "free_y") + 
  scale_color_manual(name = "",
                     values = c("s1.1 (Ref Model: No Env-Predator)" = "#ca0020",
                                "s1.4 (Env and Predator data)" = "#404040")) +
  labs(title = "", x = "", y = "") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        legend.position = "none")

alloutput

```


## Comparison

### Comparison outputs betwwen scenarios

```{r}

#read in all model runs
#note if cover=T you need a hessian; if covar=F you do not need a hessian
biglist <- SSgetoutput(keyvec = NULL,
                       dirvec = c(dir1.1,
                                  dir1.2,
                                  dir1.3,
                                  dir1.4),
                       getcovar = F)
summaryoutputall <- SSsummarize(biglist)
```

```{r}
legend.labels <- c('s1.1','s1.2','s1.3', 's1.4')

#read in all model runs
#note if cover=T you need a hessian; if covar=F you do not need a hessian
biglist <- SSgetoutput(keyvec = NULL, dirvec = c(dir1.1,
                                                 dir1.2,
                                                 dir1.3,
                                                 dir1.4),	getcovar = F)

#create summary of model runs from list above
summaryoutput <- SSsummarize(biglist)

tablelike <- SStableComparisons(summaryoutput,
                   likenames = c("TOTAL",
                                 "Survey",
                                 "Length_comp",
                                 "Age_comp",
                                 "priors",
                                 "Size_at_age"), 
                   names = c("Recr_Virgin",
                             "R0",
                             "SSB_Virg", 
                             "Bratio_2020",
                             "SPRratio_2020"),
                   digits = NULL,
                   modelnames = legend.labels,
                   csv=TRUE,
                   csvdir = "/Users/mauriciomardones/DOCAS/SA_Krill",
                   csvfile = "parameter_comparison_table.csv", verbose = TRUE,
                   mcmc = FALSE)
kbl(tablelike, booktabs = T,
    format = "html",
    caption = "")  %>% 
    kable_styling(latex_options = "scale_down")
```

```{r eval=FALSE}
#read in all model runs
#note if cover=T you need a hessian; if covar=F you do not need a hessian
biglist1_4 <- SSgetoutput(keyvec = NULL,
                       dirvec = c(
                                  dir1.1,
                                  dir1.2,
                                  dir1.3,
                                  dir1.4),
                       getcovar = F)

#create summary of model runs from list above
summaryoutput1_4 <- SSsummarize(biglist1_4)

SSplotComparisons(summaryoutput1_4,
                  legendlabels = c("s1.1 (Ref Model)",
                 "s1.2",
                 "s1.3",
                 "s1.4"),
                 filenameprefix = "COM1",
                 labels = c("Year", 
                            "Spawning biomass (t)",
                            "Relative spawning biomass", 
                            "Age-0 recruits (1,000s)",
                            "Recruitment deviations", 
                            "Index", "Log index", 
                            "1 - SPR", 
                            "Density",
                            "Management target", 
                            "Minimum stock size threshold",
                            "Spawning output",
                            "Harvest rate"),
                 png=TRUE,
                 plotdir=Figs)
```

Comparison between select models `Ref Model: No Env-Predator` and `S1.1 w/ Env and Predator data`

```{r eval=FALSE}
#read in all model runs
#note if cover=T you need a hessian; if covar=F you do not need a hessian
biglist14 <- SSgetoutput(keyvec = NULL,
                       dirvec = c(
                                  dir1.1,
                                  dir1.4),
                       getcovar = F)

#create summary of model runs from list above
summaryoutput14 <- SSsummarize(biglist14)

comp14 <- SSplotComparisons(summaryoutput14,
                  legendlabels = c("Ref Model: No Env-Predator",
                 "S1.1 w/ Env and Predator data"),
                  filenameprefix = "COM2",
                 labels = c("Year", 
                            "Spawning biomass (t)",
                            "Relative spawning biomass", 
                            "Age-0 recruits (1,000s)",
                            "Recruitment deviations", 
                            "Index", "Log index", 
                            "1 - SPR", 
                            "Density",
                            "Management target", 
                            "Minimum stock size threshold",
                            "Spawning output",
                            "Harvest rate"),
                 png=TRUE,plotdir=Figs)
```

Antoher comparision in `R0` (Natural log of virgin recruitment level)


```{r}
r0_vals <- data.frame(
  model = c("s1.1", "s1.2", "s1.3", "s1.4"),
  value = c(base.model1.1$jitter_info[1,1],
            base.model1.2$jitter_info[2,1],
            base.model1.3$jitter_info[1,1],
            base.model1.4$jitter_info[2,1]),
  cv = c(base.model1.1$jitter_info[1,6],
         base.model1.2$jitter_info[2,6],
         base.model1.3$jitter_info[1,6],
         base.model1.4$jitter_info[2,6])
)

x_vals <- seq(10, 30, length.out = 1000)

dens_data <- r0_vals %>%
  rowwise() %>%
  mutate(sd =  cv,  
         density = list(data.frame(
           x = x_vals,
           y = dnorm(x_vals, mean = value, sd = sd),
           model = model
         ))) %>%
  select(density) %>%
  unnest(cols = c(density))

colores <- c("s1.1" = "#ca0020", 
             "s1.2" = "#f4a582", 
             "s1.3" = "#bababa", 
             "s1.4" = "#404040")

r0plot <- ggplot(dens_data, aes(x = x, y = y, color = model, fill = model)) +
  geom_area(alpha = 0.5, position = "identity") +
  geom_line(size = 1) +
  scale_color_manual(values = colores) +
  scale_fill_manual(values = colores) +
  labs(title = "",
       x = "Natural log of virgin recruitment level (R0)", y = "Density") +
  theme_bw() +
  xlim(16, 22)
r0plot
```

### Comparsion in sd long term time series forecasting

```{r fig.height=5, fig.width=8}

errt <- data.frame(
  Bio_all_1 = base.model1.1$timeseries$Bio_all,
  Bio_all_2 = base.model1.2$timeseries$Bio_all,
  Bio_all_3 = base.model1.3$timeseries$Bio_all,
  Bio_all_4 = base.model1.4$timeseries$Bio_all,
  
  Bio_smry_1 = base.model1.1$timeseries$Bio_smry,
  Bio_smry_2 = base.model1.2$timeseries$Bio_smry,
  Bio_smry_3 = base.model1.3$timeseries$Bio_smry,
  Bio_smry_4 = base.model1.4$timeseries$Bio_smry,
  
  SpawnBio_1 = base.model1.1$timeseries$SpawnBio,
  SpawnBio_2 = base.model1.2$timeseries$SpawnBio,
  SpawnBio_3 = base.model1.3$timeseries$SpawnBio,
  SpawnBio_4 = base.model1.4$timeseries$SpawnBio,
  
  Recruit_0_1 = base.model1.1$timeseries$Recruit_0,
  Recruit_0_2 = base.model1.2$timeseries$Recruit_0,
  Recruit_0_3 = base.model1.3$timeseries$Recruit_0,
  Recruit_0_4 = base.model1.4$timeseries$Recruit_0
)


err_long <- errt %>%
  pivot_longer(cols = everything(), 
               names_to = "Variable_Model",
               values_to = "Value")

err_long <- err_long %>%
  extract(Variable_Model, into = c("Variable", "Model"), 
          regex = "(.+?)_(\\d+)$")

err_long <- err_long %>%
  mutate(Model = case_when(
    Model == "1" ~ "Ref Model: No Env-Predator",
    Model == "2" ~ "S1.1 w/ Predator data",
    Model == "3" ~ "S1.1 w/ Env data",
    Model == "4" ~ "S1.1 w/ Env and Predator data",
    TRUE ~ Model
  ))

ggplot(err_long, aes(x = Model, y = Value, fill = Model)) +
  geom_boxplot(width = 0.5, outlier.shape = NA,
               outliers = F) +
  #geom_jitter(width = 0.2, alpha = 0.1) +
  facet_wrap(~Variable, ncol=4, 
             scales = "free_y") +
  scale_fill_manual(name = "Scenario",
                    values = c("#ca0020", "#f4a582", "#bababa", "#404040")) +
  labs(title = "",
       x = "",
       y = "") +
  theme_few() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        legend.position = "none")
```

Statistical diferences

```{r}
# Extraer los datos
spawn_bio_1 <- base.model1.1$timeseries$SpawnBio
spawn_bio_4 <- base.model1.4$timeseries$SpawnBio

# Prueba de normalidad (Shapiro-Wilk)
shapiro.test(spawn_bio_1)
shapiro.test(spawn_bio_4)

# Prueba de igualdad de varianzas (Levene)
library(car)
leveneTest(c(spawn_bio_1, spawn_bio_4), 
           group = rep(c("Model 1.1", "Model 1.4"), each = length(spawn_bio_1)))

# Prueba t de Student (si los datos son normales)
t.test(spawn_bio_1, spawn_bio_4, var.equal = TRUE)

# Prueba de Wilcoxon (si los datos no son normales)
wilcox.test(spawn_bio_1, spawn_bio_4)

```


**Two-Sample t-Test**  

- **t-value:** 4.2682  
- **Degrees of Freedom (df):** 72  
- **p-value:** 5.915e-05 (very small)  
- **95% Confidence Interval for the Mean Difference:** [1,387,011; 3,818,074]  
- **Sample Means:**  
  - Model 1.1: **4,600,304**  
  - Model 1.4: **1,997,762**  

Since the **p-value (5.915e-05) is much smaller than 0.05**, we reject the null hypothesis. This means there is **a statistically significant difference** between the spawning biomass predicted by Model 1.1 and Model 1.4. The confidence interval suggests that Model 1.1 tends to estimate a higher spawning biomass than Model 1.4, with an estimated difference between **1.39 and 3.82 million tons**.  

**Wilcoxon Rank Sum Test (Mann-Whitney U Test)**  

- **W-statistic:** 1256  
- **p-value:** 6.698e-10 (extremely small)  
- **Warning:** "Cannot compute exact p-value with ties" (indicates that some values are repeated)  

The **Wilcoxon test also rejects the null hypothesis** with an extremely small **p-value (6.698e-10)**. This confirms that the distribution of spawning biomass values is significantly different between the two models. Since the Wilcoxon test does not assume normality, it supports the conclusion from the **t-test**, reinforcing that 

Both tests strongly indicate that **Model 1.1 and Model 1.4 produce significantly different estimates of spawning biomass**. Specifically, **Model 1.1 predicts higher values**. This suggests that the factors or assumptions included in Model 1.4 result in **lower spawning biomass estimates**, which could have important implications for stock assessment and fisheries management.


### Autocorrelation in Recruit 

To evaluate the temporal correlation structure of krill recruitment under different model configurations, we performed an Autocorrelation Function (ACF) analysis. The ACF measures the correlation between observations at different time lags, helping to assess whether recruitment estimates exhibit persistence or randomness across time.

The analysis was conducted on recruitment estimates derived from four model scenarios:

A reference model without environmental or predator influences (Ref Model: No Env-Predator).
A model incorporating predator data (S1.1 w/ Predator data).
A model incorporating environmental data (S1.1 w/ Env data).
A model incorporating both environmental and predator data (S1.1 w/ Env and Predator data).
Each model's residuals were extracted, and the autocorrelation function (ACF) was computed for a time lag range of up to 15 years. The dashed blue lines in the plots represent the 95% confidence intervals, indicating the threshold beyond which correlation values are statistically significant. If autocorrelation values remain within this range, it suggests that the recruitment estimates behave as a random process with no significant dependence on past values. Conversely, autocorrelation values exceeding these bounds indicate recruitment persistence or cyclic patterns.

```{r}
recs1 <- base.model1.1$recruit$pred_recr
recs2 <- base.model1.2$recruit$pred_recr
recs3 <- base.model1.3$recruit$pred_recr
recs4 <- base.model1.4$recruit$pred_recr

p1 <- ggAcf(base.model1.1$recruit$pred_recr) +
  ggtitle("Ref Model: No Env-Predator") +
  theme_few()
p2 <- ggAcf(base.model1.2$recruit$pred_recr) +
  ggtitle("S1.1 w/ Predator data") +
  theme_few()
p3 <- ggAcf(base.model1.3$recruit$pred_recr) +
  ggtitle("S1.1 w/ Env data") +
  theme_few()
p4 <- ggAcf(base.model1.4$recruit$pred_recr) +
  ggtitle("S1.1 w/ Env and Predator data") +
  theme_few()

ggarrange(p1,
          p2,
          p3,
          p4, ncol=1, nrow = 4,
          common.legend = T)

```

The ACF plots indicate that the reference model (without environmental or predator data) exhibits weak but noticeable positive autocorrelation at certain lags, suggesting some degree of recruitment dependence over time. However, this autocorrelation does not appear strong or systematic.

The model incorporating only predator data shows a slight reduction in autocorrelation magnitude, suggesting that predator-driven recruitment variability may have captured part of the temporal structure in the data.

The model with only environmental data exhibits a further reduction in autocorrelation, implying that environmental variability explains a larger portion of recruitment trends than predator effects alone.

Finally, the model that includes both environmental and predator data presents the lowest autocorrelation values, with nearly all bars remaining within the confidence bounds. This suggests that incorporating both factors provides the most effective explanation of recruitment fluctuations, reducing unexplained temporal structure.

Overall, these results indicate that recruitment variability is at least partially driven by environmental and predator influences, and models integrating these factors provide more robust and independent recruitment estimates, minimizing systematic dependencies over time.

### Recruit deviation

```{r, warning=FALSE}
dev1 <- base.model1.1$recruit[8:35,c(1,7)]%>% 
  mutate(Serie = "s1.1")
dev2 <- base.model1.2$recruit[8:35,c(1,7)] %>% 
  mutate(Serie = "s1.2")
dev3 <- base.model1.3$recruit[8:35,c(1,7)] %>% 
  mutate(Serie = "s1.3")
dev4 <- base.model1.4$recruit[8:35,c(1,7)] %>% 
  mutate(Serie = "s1.4")


data_list <- list(dev1, 
                  dev2,
                  dev3,
                  dev4)
titles <- c("Ref Model: No Env-Predator",
                 "S1.1 w/ Predator data",
                 "S1.1 w/ Env data",
                 "S1.1 w/ Env and Predator data")
# Lista para almacenar los gráficos
plots <- list()

for (i in 1:length(data_list)) {
  p <- ggplot(data_list[[i]], aes(x = Yr, y = dev)) +
    geom_point() +
    stat_smooth(method = "loess", 
                span = 0.15, 
                se = T) +
    geom_hline(yintercept = 0,
               col="red",
               linetype = "dotdash")+
    ggtitle(titles[i]) +
    theme(axis.text.x = element_text(angle = 90,
                                   hjust = 1))+
    ylim(-2,2)+
    theme_few()
  plots[[i]] <- p
}

# Organizar los gráficos con ggarrange
combined_plot <- ggarrange(plotlist = plots, ncol = 2, nrow = 2)

combined_plot
```



```{r}
df1 <- base.model1.1$likelihoods_used %>%
  select(values) %>%
  mutate(Componente = rownames(base.model1.1$likelihoods_used), Modelo = "s1.1")
df2 <- base.model1.2$likelihoods_used %>%
  select(values) %>%
  mutate(Componente = rownames(base.model1.2$likelihoods_used), Modelo = "s1.2")
df3 <- base.model1.3$likelihoods_used %>%
  select(values) %>%
  mutate(Componente = rownames(base.model1.3$likelihoods_used), Modelo = "s1.3")
df4 <- base.model1.4$likelihoods_used %>%
  select(values) %>%
  mutate(Componente = rownames(base.model1.4$likelihoods_used), Modelo = "s1.4")

df <- bind_rows(df1, 
                df2,
                df3,
                df4)
ggplot(df %>% 
         filter(values > 1), aes(x = Componente, y = values, fill = Modelo)) +
  geom_col(position = position_dodge(), width = 0.7) +  
  theme_few() +
  coord_flip() +  
  scale_fill_manual(name= "Scenario",
                    values = c("s1.1" = '#ca0020',
                               "s1.2" = '#f4a582', 
                               "s1.3" = '#bababa',
                               "s1.4" = '#404040')) + 
  labs(x = "",
       y = "Log-Normal Likelihood", 
       title = "") 
```
This bar plot presents the log-normal likelihood contributions of different model components in an Antarctic krill stock assessment under four scenarios (s1.1, s1.2, s1.3, and s1.4).  


**TOTAL Likelihood:** The overall likelihood is highest for scenario s1.4 (black), followed by s1.2 (light orange). This suggests that s1.4 provides the best overall model fit, while s1.1 (dark red) has the lowest likelihood, indicating a poorer fit.  

**Survey Component:** The survey likelihood follows a similar pattern, with s1.4 and s1.2 showing better agreement with the data compared to s1.1.  

**Recruitment, Parameter Priors, and Parameter Deviations:** These components contribute minimally to the total likelihood, indicating that they do not heavily influence model fit differences.  

**Length Composition:** There is some variation in this component, with s1.2 showing a larger contribution than s1.1 and s1.3.  

Scenario s1.4 seems to fit the data best, followed closely by s1.2. Scenario s1.1 has the poorest likelihood, meaning it is the least supported by the data. If s1.4 includes both environmental and predator effects, this suggests that incorporating ecosystem variables significantly improves the stock assessment model for Antarctic krill.

### Explotation Rate

```{r}
df1 <- base.model1.1$exploitation[, c(1, 4)] %>% mutate(model = "s1.1")
df2 <- base.model1.2$exploitation[, c(1, 4)] %>% mutate(model = "s1.2")
df3 <- base.model1.3$exploitation[, c(1, 4)] %>% mutate(model = "s1.3")
df4 <- base.model1.4$exploitation[, c(1, 4)] %>% mutate(model = "s1.4")


df_all <- bind_rows(df1, df2, df3, df4)
colnames(df_all) <- c("year", "exploitation_rate", "model")

colores <- c("s1.1" = "#ca0020", 
             "s1.2" = "#f4a582", 
             "s1.3" = "#bababa", 
             "s1.4" = "#404040")
ggplot(df_all, aes(x = year, y = exploitation_rate, color = model)) +
  geom_line() +
  geom_point() +
  scale_color_manual(values = colores) +
  labs(
    title = "Exploitation Rate Over Time",
    x = "Year",
    y = "Exploitation Rate",
    color = "Model"
  ) +
  theme_minimal()
```


### Platoons analisis

```{r}
# Definir los valores del eje x
x_values <- seq(0, 7, by=0.1)

# Definir las medias y desviaciones estándar para los valores específicos
means <- c( 2.9,  3.5, 4.1)
std_devs <- c(0.65, 0.8, 0.65)


# Crear un dataframe que contenga los valores de x, la media y las desviaciones estándar
data <- data.frame(
  x = rep(x_values, each = length(means)),
  mean = rep(means, times = length(x_values)),
  std_dev = rep(std_devs, times = length(x_values))
)

# Calcular las curvas de las desviaciones estándar
data <- data %>%
  mutate(y = exp(-((x - mean)^2) / (2 * std_dev^2)))

# Graficar con ggplot
ggplot(data, aes(x = x, y = y, linetype = as.factor(mean))) +
  geom_line(size = 1) +
  scale_linetype_manual(values = means,
                        name ="Platoon") +
  labs(x = "", y = "growth increment") +
  theme_minimal()+
  xlim(0,7)

```

### AKL

In a catch-at-length model like krill assessment the AKL matrix is modelled trought parametrization process

```{r AKL, out.width='60%',  fig.cap="Representation of ALK Matrix to krill in 48.1"}
alk_matrix <- base.model1.1$ALK[,,1] # Ajusta las dimensiones según tu matriz

# Convertir la matriz en un data.frame
alk_df <- as.data.frame(as.table(alk_matrix))

# Renombrar columnas para mayor claridad
colnames(alk_df) <- c("Length", "TrueAge", "Value")

# Asegúrate de que las columnas Length y TrueAge sean numéricas
alk_df$Length <- as.numeric(as.character(alk_df$Length))
alk_df$TrueAge <- as.numeric(as.character(alk_df$TrueAge))

# Asegúrate de que la columna Value sea numérica
alk_df$Value <- as.numeric(alk_df$Value)

# Crear el plot usando ggplot
ggplot(alk_df, aes(x = TrueAge, y = Length, fill = Value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  labs(x = "True Age",
       y = "Length",
       fill = "Value") +
  theme_bw()+
  theme(legend.position = "none")
```

### Code Repository

The repository with code to replicate this analysis can be found in this [GitHub author link](https://github.com/MauroMardones/SA_Krill)

\newpage

# REFERENCES
