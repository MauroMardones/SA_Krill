---
title: "Supplementary Information 1"
subtitle: "Assessing ecosystem effects on the stock productivity of Antarctic Krill (*Euphausia superba*): An Integrated modeling perspective"
author: "Mardones, M; Jarvis Mason, E.T.; Pinones, A.;  Santa Cruz, F.; Cárdenas, C.A"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: SA_krill.bib
#csl: apa.csl
csl: icesjournal.csl
link-citations: yes
linkcolor: blue
output:
  html_document:
    keep_md: true
    toc: true
    toc_deep: 3
    toc_float:
      collapsed: false
      smooth_scroll: false
    theme: cosmo
    fontsize: 0.9em
    linestretch: 1.7
    html-math-method: katex
    self-contained: true
    code-tools: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup1, echo=FALSE}
#rm(list = ls())
#set.seed(999)
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      fig.height = 6,
                      fig.width = 7,
                      fig.align = 'center',
                      fig.pos = "H",
                      dev = 'jpeg',
                      fig.path = "Figs/",
                      dpi = 300,
                      tidy.opts=list(width.cutoff=50), 
                      tidy=TRUE)
#XQuartz is a mess, put this in your onload to default to cairo instead
options(bitmapType = "cairo") 
# (https://github.com/tidyverse/ggplot2/issues/2655)
# Lo mapas se hacen mas rapido
```


```{r message=FALSE, eval=TRUE}
# install.packages("devtools")
# devtools::install_github("r4ss/r4ss", ref="development")
# install.packages("caTools")
# library("caTools")
# install.packages("r4ss")
library(r4ss)
library(here)
#remotes::install_github("PIFSCstockassessments/ss3diags")
library(ss3diags)
library(kableExtra)
library(doParallel) # facilita la ejecución paralela en R
detectCores()
registerDoParallel(8)
library(ggpubr)
library(tibble)
library(openxlsx)
library(ggthemes)
library(forecast)
library(tidyr)
library(mixR)
library(readxl)
library(tidyverse)
library(ggridges)
```

```{r message=FALSE, warning=FALSE, echo=TRUE}
# dir01 <- here("s01") # agreggate data (no spatial diferences)
# dir1<-here("s1") # Data strata flishery
# dir2<-here("s2") # Same 9 with areas (SubStrata) as fleet. Dif size comoposition and dif CPUE and dif survey length and biomass data by strata
# dir3<-here("s3") # without S-R
# dir4<-here("s4") # 
# dir5<-here("s5") # 
# dir6<-here("s6") # 
# dir7<-here("s7") # 2 set parametres EMM-2024/23 (Mardones)
# dir8<-here("s8") # s1 platoons
# dir9<-here("s9") # s1 w/ blocks
dir1.1<-here("s1.1") # sin predador ni ambiental
dir1.2 <- here("s1.2") # s1.1 C/ predador
dir1.3 <- here("s1.3") # s1.1 solo env
dir1.4 <- here("s1.4") # s1.2 predator and env
Figs <- here("Figs")# S
```


# OVERVIEW

This study aims to evaluate the impact of ecosystem components—such as environmental variables and predator-prey interactions—on the productivity and key population dynamics of *Euphausia superba* in Subarea 48.1. By incorporating these factors into the assessment, we analyze how krill population variables respond to ecological variability, providing insights into their resilience and potential management implications.  

Here, the **reference model** represents a baseline assessment of *Euphausia superba* population dynamics in Subarea 48.1, excluding environmental and ecological variables. This model assumes that krill productivity and population parameters are driven  by intrinsic biological processes, such as growth, mortality, and recruitment and fishery impacts without accounting for external influences like environmental variability or predation pressure. By serving as a *control scenario*, this model provides a benchmark against which the impact of ecosystem components in productivity can be evaluated, allowing for a direct comparison of how environmental and ecological factors influence krill stock dynamics.  


## Statistical Model (SS3)

Stock Synthesis (v.3.30.21)  is a widely used tool for assessing fish and invertebrate populations, including Antarctic krill. SS3 is implemented in `C++` with estimation enabled through automatic differentiation (ADMB) [@Fournier2012; @Methot2013]. In this exercise, SS3 is configured as an integrated stock assessment model, explicitly accounting for age and size structure while incorporating key ecosystem drivers. The model simulates population processes such as growth, maturity, fecundity, recruitment, movement, and mortality, while also integrating environmental variability and predator-prey relationships to refine estimates of population trends. The analysis of model outputs is conducted using R, utilizing the *r4ss* and *ss3diags* packages [@Taylor2019; @Winker2023]. By leveraging a spatially implicit, ecosystem-informed approach, this assessment provides a robust framework for evaluating krill stock dynamics under changing environmental conditions. These insights are crucial for informing sustainable management strategies in the Antarctic Peninsula region, where krill plays a foundational role in the marine food web.  

## Parameters

The following table summarizes the key parameters to conditioning the reference model, including biological, growth, and population dynamics factors.

```{r}
start1 <- SS_readstarter(file = file.path(dir1.4,
                                          "starter.ss"),
                              verbose = FALSE)
dat1 <- SS_readdat(file = file.path(dir1.4, start1$datfile),
                        verbose = FALSE)
ctl1 <-  r4ss::SS_readctl(file = file.path(dir1.4,
                                    start1$ctlfil),
                        verbose = FALSE,
                        use_datlist = TRUE, 
                   datlist = dat1)
fore1 <- r4ss::SS_readforecast(file = file.path(dir1.4, 
                                                "forecast.ss"),
                              verbose = FALSE)
# can also read in wtatage.ss for an empirical wt at age model using
# r4ss::SS_readwtatage()
```


```{r}
parbio<-ctl1$MG_parms[1:10,c(1:3,7)]
 row.names( parbio)<-c("Nat M",
                       "Lmin", 
                       "Lmax",
                       "VonBert K",
                       "CV young",
                       "CV old", 
                       "Wt a", 
                       "Wt b",
                       "L50%", 
                       "Mat slope")

 SRpar<-ctl1$SR_parms[1:5,c(1:3,7)]
 Qpar<-ctl1$Q_parms[1:2,c(1:3,7)]
 Selpar<-ctl1$size_selex_parms[1:22,c(1:3,7)]
 parInit<-as.data.frame(rbind(parbio,SRpar,Qpar,Selpar))

wb <- createWorkbook()

addWorksheet(wb, "parameters")
writeData(wb, "parameters", parInit)

# Guardar el workbook
#saveWorkbook(wb, "DataKrill.xlsx", overwrite = TRUE)

```

```{r}
parInit %>%
  mutate(across(where(is.numeric), ~ round(., 3)))  %>% 
  kbl(booktabs = TRUE,
      format = "html",
      position = "ht!",
      caption = "Input parameters for the initial SS3 model of krill. Each parameter line contains a minimum value (LO), maximum value (HI), and initial value (INIT). If the phase (PHASE) for the parameter is negative, the parameter is fixed as input") %>%
  kable_paper("hover", full_width = TRUE) %>%  # Expande el ancho
  kable_styling(latex_options = c("striped", "condensed"),
                full_width = TRUE,  # Expande la tabla al ancho completo
                font_size = 12,
                html_font = "Times New Roman") %>%  # Aplica la fuente Times New Roman
  pack_rows(index = c("Natural Mortality" = 1,
                      "Growth" = 5,
                      "relationship Length-Weigth" = 2,
                      "Maturity" = 2,
                      "S-R relation" = 5,
                      "Catchability" = 2,
                      "Selectivity" = 4))



```
Source of data inpit

```{r fig.width=9, fig.height=4}
dat1_std <- dat1$CPUE %>%
  mutate(
    # Standardize 'obs' between 0 and 1
    index_std = (obs - min(obs)) / (max(obs) - min(obs)),
    
    # Recode 'index' values
    index = recode(index,
                   `1` = "FISHERYBS",
                   `2` = "FISHERYEI",
                   `3` = "FISHERYGS",
                   `4` = "FISHERYJOIN",
                   `5` = "FISHERYSSIW",
                   `6` = "SURVEYBS",
                   `7` = "SURVEYEI",
                   `8` = "SURVEYGS",
                   `9` = "SURVEYJOIN",
                   `10` = "SURVEYSSIW",
                   `11` = "PREDATOR"),
    
    # Convert 'index' to a factor with specific levels
    index = factor(index, levels = c("FISHERYBS", 
                                     "FISHERYEI", 
                                     "FISHERYGS",
                                     "FISHERYJOIN", 
                                     "FISHERYSSIW", 
                                     "SURVEYBS", 
                                     "SURVEYEI", 
                                     "SURVEYGS", 
                                     "SURVEYJOIN", 
                                     "SURVEYSSIW", 
                                     "PREDATOR")),
    
    # Assign categories based on 'index'
    category = case_when(
      index %in% c("FISHERYBS", "FISHERYEI", "FISHERYGS", "FISHERYJOIN", "FISHERYSSIW") ~ "Fishery",
      index %in% c("SURVEYBS", "SURVEYEI", "SURVEYGS", "SURVEYJOIN", "SURVEYSSIW") ~ "Survey",
      index == "PREDATOR" ~ "Predator",
      TRUE ~ NA_character_  # In case there are any unexpected values
    )
  )


# Gráfico con 3 filas, dejando 'PREDATOR' en la última fila
ggplot(dat1_std, aes(x = year, y = index_std, group = index, color = category)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~index, scales = "free_y", ncol = 5) +  # Ajusta ncol para obtener 3 filas
  scale_color_manual(values = c("Fishery" = '#1b9e77', 
                                "Survey" = '#d95f02',
                                "Predator" = '#7570b3')) + 
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "", x = "", y = "Standard index", color = "Category")


```


```{r fig.width=10, fig.height=4, warning=FALSE}
dat_long <- dat1$lencomp %>% 
  pivot_longer(cols = starts_with("l"),  
               names_to = "Talla",  
               values_to = "Frecuencia") %>% 
  mutate(Talla = as.numeric(sub("l", "", Talla))) %>%  
  mutate(FltSvy = recode(FltSvy,
                         `1` = "FISHERYBS",
                         `2` = "FISHERYEI",
                         `3` = "FISHERYGS",
                         `4` = "FISHERYJOIN",
                         `5` = "FISHERYSSIW",
                         `6` = "SURVEYBS",
                         `7` = "SURVEYEI",
                         `8` = "SURVEYGS",
                         `9` = "SURVEYJOIN",
                         `10` = "SURVEYSSIW",
                         `11` = "PREDATOR")) %>%
  mutate(FltSvy = factor(FltSvy, levels = c("FISHERYBS", 
                                            "FISHERYEI", 
                                            "FISHERYGS",
                                            "FISHERYJOIN", 
                                            "FISHERYSSIW",
                                            "SURVEYBS",
                                            "SURVEYEI", 
                                            "SURVEYGS", 
                                            "SURVEYJOIN",
                                            "SURVEYSSIW",
                                            "PREDATOR"))) 

mean_talla <- dat_long %>%
  group_by(FltSvy) %>%
  summarise(mean_Talla = mean(Talla, na.rm = TRUE))


dat_expanded <- dat_long %>%
  uncount(weights = Frecuencia)

ggplot(dat_expanded, aes(x = Talla, y = as.factor(Yr), fill = FltSvy)) +
  geom_density_ridges_gradient(scale = 1.8,
                               rel_min_height = 0.001,
                               col = "black",
                               alpha=0.4) +
  facet_wrap(~FltSvy, ncol = 10) +
  geom_vline(data = mean_talla, aes(xintercept = mean_Talla), 
             color = "red", 
             size = 0.7,
             alpha = 0.5) +
  scale_fill_manual(values = c("FISHERYBS" = '#1b9e77',  
                               "FISHERYEI" = '#1b9e77',  
                               "FISHERYGS" = '#1b9e77',
                               "FISHERYJOIN" = '#1b9e77',  
                               "FISHERYSSIW" = '#1b9e77',  
                               "SURVEYBS" = '#d95f02',  
                               "SURVEYEI" = '#d95f02',  
                               "SURVEYGS" = '#d95f02',  
                               "SURVEYJOIN" = '#d95f02',  
                               "SURVEYSSIW" = '#d95f02',  
                               "PREDATOR" = '#7570b3')) +  
  labs(title = "",
       x = "cm",
       y = "",
       fill = "Category") +
  theme_few()+
  theme(legend.position = "none")

```

## Scenarios

In Table 1 we have ten scenarios to test different option in modeling
about main consideration in assessment of krill population.

| Scenario | Description                                                                          |
|:------------:|:---------------------------------------------------------|
|    s1.1  | Spatial data without environmental and predator components                           |
|    s1.2  | "s1.1" with predator components                                                      |
|    s1.3  | "s1.1" with environmental variable                                                  |
|    s1.4  | "s1.1" w/ both, predator fleet and environmental variable                           |



### Run Models

```{r eval=FALSE, message=F, include=FALSE, echo=TRUE}
directorios <- c("s1.1",
                 "s1.2",
                 "s1.3",
                 "s1.4")  

for (dir in directorios) {
  r4ss::run(
    dir = dir,
    exe = "ss_osx",
    skipfinished = FALSE,
    show_in_console = TRUE
  )
}
```

```{r eval=FALSE, message=F, include=FALSE}
#OR with rby separate
#shell(cmd="ss") # run SS to windows
# or 
r4ss::run(
  dir = dir1.1,
  exe = "ss_osx",
  skipfinished = FALSE, 
  show_in_console = TRUE 
)


# s1.2
r4ss::run(
  dir = dir1.2,
  exe = "ss_osx",
  skipfinished = FALSE, 
  show_in_console = TRUE 
)


# s1.3
r4ss::run(
  dir = dir1.3,
  exe = "ss_osx",
  skipfinished = FALSE, 
  show_in_console = TRUE 
)


# s1.4
r4ss::run(
  dir = dir1.4,
  exe = "ss_osx",
  skipfinished = FALSE, 
  show_in_console = TRUE 
)
```



```{r message=F, include=FALSE}
#s1.1
base.model1.1 <- SS_output(dir=dir1.1,
                        covar=T,
                        forecast=T)
#s1.2
base.model1.2 <- SS_output(dir=dir1.2,
                        covar=T,
                        forecast=T)
#s1.3
base.model1.3 <- SS_output(dir=dir1.3,
                        covar=T,
                        forecast=T)
#s1.4
base.model1.4 <- SS_output(dir=dir1.4,
                        covar=T,
                        forecast=T)

```


Data used en both (spatial and No spatial models) `s1.1`

```{r }
SSplotData(base.model1.1, 
           subplots = 1,
           pheight = 15)
```

and `s1.4`

```{r }
SSplotData(base.model1.4, 
           subplots = 1,
           pheight = 15)
```

# RESULTS

```{r eval=FALSE, message=F, include=FALSE}
# Definir la ruta de la carpeta
folder_path <- "s1.1/plots"
# Verificar si la carpeta existe y eliminarla
if (dir.exists(folder_path)) {
  unlink(folder_path, recursive = TRUE)
  message("La carpeta 'plots' ha sido eliminada.")
} else {
  message("La carpeta 'plots' no existe.")
}

SS_plots(base.model1.1, 
         uncertainty = TRUE,
         datplot = T, 
         png=T, 
         aalresids = F,
         btarg=0.75, 
         minbthresh=0.2, 
         forecast=T)

# S 1.2

# Definir la ruta de la carpeta
folder_path2 <- "s1.2/plots"
# Verificar si la carpeta existe y eliminarla
if (dir.exists(folder_path2)) {
  unlink(folder_path2, 
         recursive = TRUE)
  message("La carpeta 'plots' ha sido eliminada.")
} else {
  message("La carpeta 'plots' no existe.")
}

SS_plots(base.model1.2, 
         uncertainty = TRUE,
         datplot = T, 
         png=T, 
         aalresids = F,
         btarg=0.75, 
         minbthresh=0.2, 
         forecast=T)

# S 1.3

# Definir la ruta de la carpeta
folder_path3 <- "s1.3/plots"
# Verificar si la carpeta existe y eliminarla
if (dir.exists(folder_path3)) {
  unlink(folder_path3, 
         recursive = TRUE)
  message("La carpeta 'plots' ha sido eliminada.")
} else {
  message("La carpeta 'plots' no existe.")
}

SS_plots(base.model1.3, 
         uncertainty = TRUE,
         datplot = T, 
         png=T, 
         aalresids = F,
         btarg=0.75, 
         minbthresh=0.2, 
         forecast=T)


# S 1.4

# Definir la ruta de la carpeta
folder_path4 <- "s1.4/plots"
# Verificar si la carpeta existe y eliminarla
if (dir.exists(folder_path4)) {
  unlink(folder_path4, 
         recursive = TRUE)
  message("La carpeta 'plots' ha sido eliminada.")
} else {
  message("La carpeta 'plots' no existe.")
}

SS_plots(base.model1.4, 
         uncertainty = TRUE,
         datplot = T, 
         png=T, 
         aalresids = F,
         btarg=0.75, 
         minbthresh=0.2, 
         forecast=T)
```



Main Variables poulation in `s1.1` scenario

```{r}
SSplotTimeseries(base.model1.1,
                 subplot = 1)
SSplotTimeseries(base.model1.1,
                 subplot = 9)
SSplotTimeseries(base.model1.1,
                 subplot = 7)
```

Main Variables poulation in `s1.2` scenario

```{r}
SSplotTimeseries(base.model1.2,
                 subplot = 1)
SSplotTimeseries(base.model1.2,
                 subplot = 9)
SSplotTimeseries(base.model1.2,
                 subplot = 7)
```

Main Variables poulation in `s1.3` scenario

```{r}
SSplotTimeseries(base.model1.3,
                 subplot = 1)
SSplotTimeseries(base.model1.3,
                 subplot = 9)
SSplotTimeseries(base.model1.3,
                 subplot = 7)
```


Main Variables poulation in `s1.4` scenario

```{r}
SSplotTimeseries(base.model1.4,
                 subplot = 1)
SSplotTimeseries(base.model1.4,
                 subplot = 9)
SSplotTimeseries(base.model1.4,
                 subplot = 7)
```
Selectivity

```{r}
SSplotSelex(base.model1.1,
            subplots = 1)
SSplotSelex(base.model1.2,
            subplots = 1)
SSplotSelex(base.model1.3,
            subplots = 1)
SSplotSelex(base.model1.4,
            subplots = 1)
```



```{r}
SSplotIndices(base.model1.1,
            subplots = 9)
SSplotIndices(base.model1.2,
            subplots = 9)
SSplotIndices(base.model1.3,
            subplots = 9)
SSplotIndices(base.model1.4,
            subplots = 9)
```

```{r}
SSplotComps(base.model1.1,
            subplots = 21)
SSplotComps(base.model1.2,
            subplots = 21)
SSplotComps(base.model1.3,
            subplots = 21)
SSplotComps(base.model1.4,
            subplots = 21)
```



Total biomass

```{r}
total1 <- as.data.frame(base.model1.1$timeseries[,c(2,5)]) %>%
  mutate(Serie = "Without Ecosystem Variables")
total2 <- as.data.frame(base.model1.2$timeseries[,c(2,5)]) %>% 
  mutate(Serie = "W/ Predator")
total3 <- as.data.frame(base.model1.3$timeseries[,c(2,5)]) %>% 
  mutate(Serie = "W/ Environment")
total4 <- as.data.frame(base.model1.4$timeseries[,c(2,5)]) %>% 
  mutate(Serie = "With Predator and Ecosystem")


totalbio <- rbind(total1,
              total2,
              total3,
              total4)


ggplot(totalbio, aes(x = Yr, y = Bio_all, color=Serie)) +
  geom_point() +
  geom_smooth(method = "loess",
              span=0.5, 
              se=F)+
  labs(title = "",
       x = "",
       y = "Total biomass (mt)") +
  theme_few() +
  scale_color_viridis_d(option="H",
                        name="Scenarios")+
  theme(axis.text.x = element_text(angle = 90,
                                   hjust = 1))
```

Heatmap

```{r}
df <- base.model1.4$lendbase

df1 <- df %>% 
  dplyr::filter(Pearson<5) %>% 
  dplyr::select(c(1, 6, 19, 16)) %>% 
  dplyr::mutate(Fleet = dplyr::case_when(
    Fleet == 1 ~ "FISHERYBS",
    Fleet == 2 ~ "FISHERYEI",
    Fleet == 3 ~ "FISHERYGS",
    Fleet == 4 ~ "FISHERYJOIN",
    Fleet == 5 ~ "FISHERYSSIW",
    Fleet == 6 ~ "SURVEYBS",
    Fleet == 7 ~ "SURVEYEI",
    Fleet == 8 ~ "SURVEYGS",
    Fleet == 9 ~ "SURVEYJOIN",
    Fleet == 10 ~ "SURVEYSSIW",
    Fleet == 11 ~ "PREDATOR",
    TRUE ~ NA_character_  # Para manejar cualquier valor que no coincida
  ))

# Crea el heatmap
ggplot(df1, aes(x = Yr, y = Bin, fill = Pearson)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", 
                       mid = "white", 
                       high = "red",
                       midpoint = 0) +
  facet_wrap(Fleet~., ncol=5) + 
  labs(title = "",
       x = "",
       y = "Length (cm)",
       fill = "Pearson") +
  theme_few()+
  theme(axis.text.x = element_text(angle = 90,
                                   hjust = 1, 
                                   size = 7))

```
hexagon


```{r eval=FALSE}
# Assuming your data is in a data frame called 'lendbase'
# Filter and select relevant columns
data <- base.model1.4$lendbase %>%
  dplyr::filter(Pearson<5) %>% 
  dplyr::select(c(1, 6, 19, 16)) %>% 
  dplyr::mutate(Fleet = dplyr::case_when(
    Fleet == 1 ~ "FISHERYBS",
    Fleet == 2 ~ "FISHERYEI",
    Fleet == 3 ~ "FISHERYGS",
    Fleet == 4 ~ "FISHERYJOIN",
    Fleet == 5 ~ "FISHERYSSIW",
    Fleet == 6 ~ "SURVEYBS",
    Fleet == 7 ~ "SURVEYEI",
    Fleet == 8 ~ "SURVEYGS",
    Fleet == 9 ~ "SURVEYJOIN",
    Fleet == 10 ~ "SURVEYSSIW",
    Fleet == 11 ~ "PREDATOR",
    TRUE ~ NA_character_  # Para manejar cualquier valor que no coincida
  )) %>% 
  dplyr::select(Yr, Bin, Pearson) 

# Create the hexagonal heatmap
ggplot(data, aes(x = Yr, y = Bin)) +
  geom_hex(bins=50) +  # Hexagonal bins
  scale_fill_gradient2(low = "blue", 
                       mid = "white", 
                       high = "red", 
                       midpoint = 0) +  # Color scale
  theme_minimal() +  # Minimal theme for cleaner look
  labs(title = "Hexagonal Heatmap of Pearson Residuals",
       x = "Year",
       y = "Bin",
       fill = "Pearson Residual")  # Labels for plot and axes

```

## Diagnosis Base Model

A rigorous model diagnosis is essential to ensure the reliability and robustness of stock assessment models. The key steps for a good practice in model diagnosis include:  

1. Convergence Check: The model must reach a final convergence criterion of 1.0e-04 to ensure numerical stability and reliable parameter estimation.  

2. Residual Analysis: Both visual inspection and statistical metrics are used to evaluate model residuals, helping to detect patterns of bias or misfit.  

3. Retrospective Analysis: The Mohn’s rho parameter is used to assess the consistency of model estimates when sequentially removing recent years of data, identifying potential overestimation or underestimation trends.  

4. Likelihood Profile Analysis: This approach examines how the likelihood function behaves across a range of parameter values, providing insight into parameter uncertainty and model sensitivity.  

This framework follows the recommendations outlined by @Carvalho2021b, aiming to enhance transparency and reproducibility in model evaluation.

### Residual consistency 


Residual analysis is a critical component of model diagnostics in stock assessments. It helps evaluate the fit of the model to observed data and detect potential biases or inconsistencies. This process is applied to both length composition data and abundance indices such as CPUE (Catch Per Unit Effort) and survey-derived estimates.  

For length composition data, residuals represent the difference between observed and model-predicted length distributions. The standardized residuals are calculated as the difference between observed and expected proportions at each length bin. These residuals are plotted by year to identify systematic trends, biases, or inconsistencies in the data. Ideally, they should be randomly distributed around zero, indicating no systematic over- or underestimation.  

For abundance indices such as CPUE and fishery-independent surveys, residuals are analyzed to assess model fit and potential sources of bias. Residuals are computed as the difference between observed index values and those predicted by the model, typically standardized by dividing by the standard error to facilitate comparison across years. These residuals are then plotted over time to evaluate trends. A shaded confidence region, like the green area in the provided plot, represents expected variability, with outliers highlighted in red or other distinct markers. Persistent positive or negative residuals may indicate systematic bias in the model or data collection process.  

Statistical diagnostics are also performed to check for autocorrelation in residuals, which can indicate potential model misspecifications. When mean residual values are close to zero, the model fit is considered unbiased.  

By integrating these residual analyses for both length and abundance indices, stock assessment models can be refined, improving their reliability and increasing confidence in the assessment results.

```{r}
SSplotRunstest(base.model1.1,
               subplots = "len",
               add=T,
               plot = TRUE,
               plotdir = Figs)
SSplotRunstest(base.model1.2,
               subplots = "len",
               add=T,
               plot = TRUE,
               plotdir = Figs)
SSplotRunstest(base.model1.3,
               subplots = "len",
               add=T,
               plot = TRUE,
               plotdir = Figs)
SSplotRunstest(base.model1.4,
               subplots = "len",
               add=T,
               plot = TRUE,
               plotdir = Figs)
```


```{r}
SSplotRunstest(base.model1.1,
               subplots = "cpue",
               add=T,
               print = TRUE)
SSplotRunstest(base.model1.2,
               subplots = "cpue",
               add=T)
SSplotRunstest(base.model1.3,
               subplots = "cpue",
               add=T)
SSplotRunstest(base.model1.4,
               subplots = "cpue",
               add=T)
```

### Residual Analysis and RMSE 

Residual analysis of mean length data is a fundamental diagnostic tool in stock assessments. It helps evaluate whether the model provides an unbiased fit to the observed data and detects potential biases over time. In this figure, mean length residuals are plotted across years, differentiated by data source, including fishery-dependent (FISHERY) and fishery-independent (SURVEY) datasets, as well as predator-related observations (PREDATOR). The residuals represent the deviation of observed mean length from model-predicted values, standardized to facilitate interpretation.  

The black line represents a locally estimated scatterplot smoothing (Loess) curve, which provides a trend line to visualize systematic deviations over time. The presence of persistent positive or negative trends in the residuals may indicate biases in the growth model, selectivity assumptions, or misrepresentation of recruitment variability. The gray bars highlight periods where residual variability is particularly high, suggesting potential inconsistencies between observed and predicted size structures.  

RMSE quantifies the overall deviation between observed and predicted values, providing an aggregate measure of model fit. Lower RMSE values indicate better agreement between observed and predicted data. In fisheries stock assessment [@HurtadoF2015], RMSE thresholds for acceptable model performance typically range between *10% and 30%*, depending on the data quality and complexity of the population dynamics being modeled. Values exceeding this range suggest potential biases, requiring further investigation into the model structure, parameter estimation, or data sources.   

By analyzing residual patterns and RMSE values, the model can be refined to improve the accuracy of mean length predictions, ultimately enhancing the reliability of stock assessment outcomes and management recommendations.

```{r}
SSplotJABBAres(base.model1.1,
               subplots = "len",
               add=T)
SSplotJABBAres(base.model1.2,
               subplots = "len",
               add=T)
SSplotJABBAres(base.model1.3,
               subplots = "len",
               add=T)
SSplotJABBAres(base.model1.4,
               subplots = "len",
               add=T)
```

```{r}
SSplotJABBAres(base.model1.1,
               subplots = "cpue",
               add=T)
SSplotJABBAres(base.model1.2,
               subplots = "cpue",
               add=T)
SSplotJABBAres(base.model1.3,
               subplots = "cpue",
               add=T)
SSplotJABBAres(base.model1.4,
               subplots = "cpue",
               add=T)
```



### Comparision RMSE

```{r}
# Crear un data frame con los valores de RMSE
dfrmse <- data.frame(
  Model1.1 = as.numeric(base.model1.1$index_variance_tuning_check$RMSE),
  Model1.2 = as.numeric(base.model1.2$index_variance_tuning_check$RMSE[1:10]),
  Model1.3 = as.numeric(base.model1.3$index_variance_tuning_check$RMSE[1:10]),
  Model1.4 = as.numeric(base.model1.4$index_variance_tuning_check$RMSE[1:10])
  
)
summary(dfrmse)

# Comparación con boxplot
boxplot(dfrmse, main = "RMSE comparision", col = c('#ca0020','#f4a582','#bababa','#404040'))

# Prueba de diferencias estadísticas (ANOVA)
anova_result <- aov(as.numeric(unlist(dfrmse)) ~ rep(1:4, each=nrow(dfrmse)))
summary(anova_result)



t.test(dfrmse$Model1.1, dfrmse$Model1.2)  # Comparar Model1.1 vs Model1.2
t.test(dfrmse$Model1.1, dfrmse$Model1.3)  # Comparar Model1.1 vs Model1.3
t.test(dfrmse$Model1.2, dfrmse$Model1.3)
t.test(dfrmse$Model1.3, dfrmse$Model1.4)
# ANOVA para ver si hay diferencias globales
anova_rmse <- aov(as.numeric(unlist(dfrmse)) ~ rep(1:4, each=nrow(dfrmse)))
summary(anova_rmse)

# Prueba post-hoc si ANOVA es significativa
#TukeyHSD(anova_rmse)


dfrmse_long <- reshape2::melt(dfrmse)

ggplot(dfrmse_long, aes(x = variable, y = value, fill = variable)) +
  geom_boxplot(width=0.3) +
  geom_jitter(width = 0.01, alpha = 0.3) +
  theme_minimal() +
  scale_fill_manual(name= "Scenario",
                    values = c("Model1.1" = "#ca0020", 
                               "Model1.2" = "#f4a582", 
                               "Model1.3" = "#bababa", 
                               "Model1.4" = "#404040")) +
  labs(title = "", x = "", y = "RMSE")


```
This boxplot compares a summary of the Root Mean Square Error (RMSE) across four different models (s1.1, s1.2, s1.3, and s1.4) used to evaluate recruitment estimates of Antarctic krill. RMSE serves as an indicator of model accuracy, with lower values representing better predictive performance.

s1.1 exhibits the lowest median RMSE, suggesting it has the best overall fit among the four models. In contrast, Models 1.2, 1.3, and 1.4 show higher RMSE values, indicating comparatively lower predictive accuracy. The interquartile range (IQR) of these models is relatively similar, suggesting comparable variability in RMSE across models. Additionally, s1.1 has an outlier above 1.5 RMSE, which could indicate a case where the model's predictions deviated significantly from observed values.

Overall, this analysis highlights that incorporating different environmental or predator-related variables in the models impacts their predictive ability. The differences in RMSE suggest that some models may overfit or underfit the recruitment patterns of krill, emphasizing the need to refine model selection based on ecological and statistical considerations.


### Relationship Stock-Recruit

\[
R = \frac{aS}{1 + bS}
\]

Where:  
- \( S \) is the spawning stock biomass.  
- \( R \) is the predicted recruitment.  
- \( a \) is the maximum recruitment capacity.  
- \( b \) regulates density dependence (higher values of \( b \) result in a lower plateau).  

The blue line will show the asymptotic curve that describes the relationship between spawning stock biomass and recruitment.


```{r}
# First dataset
l <- base.model1.1$recruit$SpawnBio  
m <- base.model1.1$recruit$pred_recr 
yr <- base.model1.1$timeseries$Yr[1:35]     

# Second dataset
leco <- base.model1.4$recruit$SpawnBio  
meco <- base.model1.4$recruit$pred_recr 
yreco <- base.model1.4$timeseries$Yr[1:35]  

# Beverton-Holt models
bev_holt_model1 <- nls(m ~ (a * l) / (1 + b * l), 
                        start = list(a = max(m), b = 1 / max(l)), 
                        data = data.frame(l, m))

bev_holt_model2 <- nls(meco ~ (a * leco) / (1 + b * leco), 
                        start = list(a = max(meco), b = 1 / max(leco)), 
                        data = data.frame(leco, meco))

# Predictions
l_pred1 <- seq(min(l), max(l), length.out = 100)
m_pred1 <- predict(bev_holt_model1, newdata = data.frame(l = l_pred1))

l_pred2 <- seq(min(leco), max(leco), length.out = 100)
m_pred2 <- predict(bev_holt_model2, newdata = data.frame(leco = l_pred2))

# Create data frames for plotting
data1 <- data.frame(l = l, m = m, yr = yr, Group = "s1.1")
data2 <- data.frame(l = leco, m = meco, yr = yreco, Group = "s1.4")

# Plot
ggplot() +
  geom_point(data = data1, aes(x = l, y = m, shape = Group),
             size = 1.5) +
  geom_text(data = data1, aes(x = l, y = m, label = yr), 
            hjust = 0.5, vjust = -1.5, size = 3) +  
  geom_line(data = data.frame(l = l_pred1, m = m_pred1), 
            aes(x = l, y = m), linewidth = 1,
            col="blue") +
  geom_point(data = data2, aes(x = l, y = m, shape = Group), 
             size = 1.5) +
  geom_text(data = data2, aes(x = l, y = m, label = yr), 
            hjust = 0.5, vjust = 1.5, size = 3) +
  geom_line(data = data.frame(l = l_pred2, m = m_pred2), 
            aes(x = l, y = m), linewidth = 1,
            col="blue") +
  scale_shape_manual(values = c("s1.1" = 5, 
                                "s1.4" = 4)) +
  theme_few() +
  ylim(0, max(m, meco)) +
  labs(x = "SSB", y = "Recruit", 
       title = "", shape = "Model") +
  theme(legend.position = "bottom")

```

### Retrospective Analysis in Model Evaluation

Retrospective analyses provide insights into the differences in estimation patterns (underestimation or overestimation) among the models evaluated. These analyses assess the consistency and reliability of stock assessment models by systematically removing the most recent years of data and comparing the resulting estimates with the full dataset.  

In this study, we conducted a retrospective analysis to examine the sensitivity of our recruitment and spawning stock biomass (SSB) estimates to the inclusion or exclusion of recent data. By applying this approach to multiple models, we identified potential biases and evaluated the stability of the recruitment estimates over time.  

The retrospective patterns were assessed by calculating the relative error between the predictions of truncated datasets and the full dataset. These differences allowed us to detect trends in model performance, such as systematic overestimation or underestimation of key population parameters. Understanding these deviations is crucial for improving the robustness of the stock assessment models and ensuring more reliable projections for fisheries management.

```{r eval=FALSE}
#one by one
retro(
    dir = dir1.1,
    oldsubdir = "",
    newsubdir = "Retrospective",
    years = 0:-4,
    exe = "ss_osx",
    extras = "-nox",
    skipfinished = FALSE)
```


```{r eval=FALSE}
directorios <- c("s1.1",
                 "s1.2",
                 "s1.3",
                 "s1.4")  
for (dir in directorios) {
  retro(
    dir = dir,
    oldsubdir = "",
    newsubdir = "Retrospective",
    years = 0:-5,
    exe = "ss_osx",
    extras = "-nox",
    skipfinished = FALSE
  )
}
```

```{r message=FALSE, warning=FALSE}
#stest
retroModels1.1 <- SSgetoutput(dirvec=file.path(dir1.1,
                                            "Retrospective",
                                            paste("retro",0:-4,
                                                  sep="")))

retroSummary1.1 <- SSsummarize(retroModels1.1)
#stest
retroModels1.2 <- SSgetoutput(dirvec=file.path(dir1.2,
                                            "Retrospective",
                                            paste("retro",0:-4,
                                                  sep="")))

retroSummary1.2 <- SSsummarize(retroModels1.2)
#stest
retroModels1.3 <- SSgetoutput(dirvec=file.path(dir1.3,
                                            "Retrospective",
                                            paste("retro",0:-4,
                                                  sep="")))

retroSummary1.3 <- SSsummarize(retroModels1.3)
#stest
retroModels1.4 <- SSgetoutput(dirvec=file.path(dir1.4,
                                            "Retrospective",
                                            paste("retro",0:-4,
                                                  sep="")))

retroSummary1.4 <- SSsummarize(retroModels1.4)


```

```{r}
#save(retroSummary, retroModels, file="retro5.Rdata")
retro1.1 <- SSplotRetro(retroSummary1.1,
            add=T,
            forecast = F,
            legend = T,
            verbose=F,
            subplots = c("SSB"))
retro1.2 <- SSplotRetro(retroSummary1.2,
            add=T,
            forecast = F,
            legend = T,
            verbose=F,
            subplots = c("SSB"))
retro1.3 <- SSplotRetro(retroSummary1.3,
            add=T,
            forecast = F,
            legend = T,
            verbose=F,
            subplots = c("SSB"))
retro1.4 <- SSplotRetro(retroSummary1.4,
            add=T,
            forecast = F,
            legend = T,
            verbose=F,
            subplots = c("SSB"))
```


### Hindcast Cross-Validation and Prediction Skill

The Hindcast Cross-Validation (HCxval) diagnostic in Stock Synthesis is implemented using the model outputs generated by the `r4ss::SS_doRetro()` function. This diagnostic evaluates the predictive performance of the model by comparing hindcast predictions with observed data.

To assess prediction skill, we employ the Mean Absolute Scaled Error (MASE) as a robust metric. MASE is calculated by scaling the mean absolute error of the model predictions relative to the mean absolute error of a naïve baseline prediction. Specifically, the MASE score is computed as follows:

- A MASE score greater than 1 indicates that the model’s average forecasts are less accurate than a random walk model.
- A MASE score equal to 1 suggests that the model’s accuracy is similar to that of a random walk.
- A MASE score less than 1 indicates that the model performs better than a random walk.
- A MASE score of 0.5, for example, indicates that the model’s forecasts are twice as accurate as the naïve baseline prediction, suggesting the model has predictive skill.

This approach provides a rigorous evaluation of model forecasting capabilities and helps identify improvements for model calibration.


```{r}
hci = SSplotHCxval(retroSummary1.1, 
                   add = T, 
                   verbose = F, 
                   legendcex = 0.7)
hci = SSplotHCxval(retroSummary1.2, 
                   add = T, 
                   verbose = F, 
                   legendcex = 0.7)
hci = SSplotHCxval(retroSummary1.3, 
                   add = T, 
                   verbose = F, 
                   legendcex = 0.7)
hci = SSplotHCxval(retroSummary1.4, 
                   add = T, 
                   verbose = F, 
                   legendcex = 0.7) 
```

### Kobe (status)


```{r eval=FALSE}
mvln = SSdeltaMVLN(base.model1.1,
                   Fref = "MSY", 
                   plot = TRUE,
                   addprj=TRUE,
                   virgin = TRUE,
                   mc=100)
mvln = SSdeltaMVLN(base.model1.2,
                   Fref = "MSY", 
                   plot = TRUE,
                   addprj=TRUE,
                   virgin = TRUE,
                   mc=100)
mvln = SSdeltaMVLN(base.model1.3,
                   Fref = "MSY", 
                   plot = TRUE,
                   addprj=TRUE,
                   virgin = TRUE,
                   mc=100)
mvln = SSdeltaMVLN(base.model1.4,
                   Fref = "MSY", 
                   plot = TRUE,
                   addprj=TRUE,
                   virgin = TRUE,
                   mc=100)

```
another

```{r eval=FALSE}

nanios<-base.model1.1$Kobe$Yr
SSBDep<-base.model1.1$Kobe$B.Bmsy
y_BDPR<-base.model1.1$Kobe$F.Fmsy

kobe1 <- ggplot(base.model1.1$Kobe)+
  geom_rect(aes(xmin = 0, xmax = 0.5, ymin = 0, ymax = 5),
            fill = "#E43338", alpha = 0.5) +
  geom_rect(aes(xmin = 0.5, xmax = 0.75, ymin = 0, ymax = 5),
            fill = "#F2ED23", alpha = 0.5) +
  geom_rect(aes(xmin = 0.75, xmax = 1.25, ymin = 0, ymax = 5),
            fill = "#ACC39A", alpha = 0.5) +
  geom_rect(aes(xmin = 1.25, xmax = 9, ymin = 0, ymax = 1),
            fill = "#608D68", alpha = 0.5) +
  geom_rect(aes(xmin = 1.25, xmax = 9, ymin = 1, ymax = 5),
            fill = "#808080", alpha = 0.5) +
  geom_path(aes(x=base.model1.1$Kobe$B.Bmsy,y=base.model1.1$Kobe$F.Fmsy,
                label=base.model1.1$Kobe$Yr))+
  geom_point(aes(base.model1.1$Kobe$B.Bmsy, base.model1.1$Kobe$F.Fmsy),
             lwd=2) +
  geom_hline(yintercept = 1) +
  geom_vline(xintercept = c(0.5, 0.75, 1.75, 1, 1.25), linetype=2)+
  theme_few()+
  labs(x = expression("BD/BD"[RMS]), y = expression("F/F"[RMS]))+
  # geom_text(data = texto_coords, aes(x = x, y = y, label = etiqueta),
  #            vjust = -0.5)+
  geom_text(aes(x=base.model1.1$Kobe$B.Bmsy,y=base.model1.1$Kobe$F.Fmsy,label=base.model1.1$Kobe$Yr),
            nudge_y = 0.1,size = 3,
            check_overlap = TRUE)
kobe1

```

```{r}
# model01
tablebias01 <- SShcbias(retroSummary1.1,quant="SSB",verbose=F)
tablebias01a <- SShcbias(retroSummary1.1,quant="F",verbose=F)

kbl(tablebias01, booktabs = T,format = "latex",
    caption = "Rho parameter in SSB model s01")  %>% 
    kable_styling(latex_options = "HOLD_position")

kbl(tablebias01a, booktabs = T,format = "latex",
    caption = "Rho parameter in F model s01")  %>% 
    kable_styling(latex_options = "HOLD_position")

# model2
tablebias2 <- SShcbias(retroSummary1.2,quant="SSB",verbose=F)
tablebias2a <- SShcbias(retroSummary1.2,quant="F",verbose=F)

kbl(tablebias2, booktabs = T,format = "latex",
    caption = "Rho parameter in SSB model s2")  %>% 
    kable_styling(latex_options = "HOLD_position")

kbl(tablebias2a, booktabs = T,format = "latex",
    caption = "Rho parameter in F model s2")  %>% 
    kable_styling(latex_options = "HOLD_position")

# model3
tablebias3 <- SShcbias(retroSummary1.3,quant="SSB",verbose=F)
tablebias3a <- SShcbias(retroSummary1.3,quant="F",verbose=F)

kbl(tablebias3, booktabs = T,format = "latex",
    caption = "Rho parameter in SSB model s3")  %>% 
    kable_styling(latex_options = "HOLD_position")

kbl(tablebias3a, booktabs = T,format = "latex",
    caption = "Rho parameter in F model s3")  %>% 
    kable_styling(latex_options = "HOLD_position")


# model4
tablebias4 <- SShcbias(retroSummary1.4,quant="SSB",verbose=F)
tablebias4a <- SShcbias(retroSummary1.4,quant="F",verbose=F)

kbl(tablebias4, booktabs = T,format = "latex",
    caption = "Rho parameter in SSB model s4")  %>% 
    kable_styling(latex_options = "HOLD_position")

kbl(tablebias4a, booktabs = T,format = "latex",
    caption = "Rho parameter in F model s4")  %>% 
    kable_styling(latex_options = "HOLD_position")


# Combine the results from all models into one table
tablebias_combined <- bind_rows(
  data.frame(Model = "s1.1", 
             Quant = "SSB", 
             Rho = SShcbias(retroSummary1.1, 
                            quant = "SSB", verbose = F)),
  data.frame(Model = "s1.1", 
             Quant = "F", 
             Rho = SShcbias(retroSummary1.1, 
                            quant = "F", verbose = F)),
  data.frame(Model = "s1.2", 
             Quant = "SSB", 
             Rho = SShcbias(retroSummary1.2, 
                            quant = "SSB", verbose = F)),
  data.frame(Model = "s1.2", 
             Quant = "F", 
             Rho = SShcbias(retroSummary1.2, 
                            quant = "F", verbose = F)),
  data.frame(Model = "s1.3", 
             Quant = "SSB", 
             Rho = SShcbias(retroSummary1.3, 
                            quant = "SSB", verbose = F)),
  data.frame(Model = "s1.3", 
             Quant = "F", 
             Rho = SShcbias(retroSummary1.3, 
                            quant = "F", verbose = F)),
  data.frame(Model = "s1.4", 
             Quant = "SSB", 
             Rho = SShcbias(retroSummary1.4, 
                            quant = "SSB", verbose = F)),
  data.frame(Model = "s1.4",
             Quant = "F", 
             Rho = SShcbias(retroSummary1.4, 
                            quant = "F", verbose = F))
)

kbl(tablebias_combined, booktabs = T, 
    format = "html", 
    caption = "Rho parameter by model and quantity (SSB and F)") %>% 
  kable_styling(latex_options = "HOLD_position")


```

### Likelihood tables

```{r}

like1 <- base.model1.1$likelihoods_used
like1$model <- rep("s1.1", nrow(like1))
like1 <- rownames_to_column(like1, var = "Description")
like2 <- base.model1.2$likelihoods_used
like2$model <- rep("s1.2", nrow(like2))
like2 <- rownames_to_column(like2, var = "Description")
like3 <- base.model1.3$likelihoods_used
like3$model <- rep("s1.3", nrow(like3))
like3 <- rownames_to_column(like3, var = "Description")
like4 <- base.model1.4$likelihoods_used
like4$model <- rep("s1.4", nrow(like4))
like4 <- rownames_to_column(like4, var = "Description")

totalike <- rbind(like1,
                   like2,
                   like3,
                   like4)

```

```{r}
ggplot(totalike)+
  geom_bar( aes(y = reorder(Description, values), 
                x =log(values)),
            stat = "identity", position = "dodge") +
  theme_few() +
  facet_wrap(.~model, 
             ncol=4)+
  labs(y="",
       x="Log Likelihood")+
  theme(axis.text.x = element_text(angle = 90, 
                                   hjust = 1))+
  xlim(0,30)
```

### Likelihood Profile 

```{r eval=FALSE, echo = FALSE}
# 1. Identificar el directorio donde se encuentra el modelo base ----
dirname.model.run <- here("s1.2")
# 2. Crear un nuevo directorio para el "Perfil_Verosimilitud"  
dirname.R0.profile <- here("s1.2",
                           "Perfil_Verosimilitud")
dir.create(path=dirname.R0.profile, 
           showWarnings = TRUE, 
           recursive = TRUE)
# 3. Crear un subdirectorio llamado "plots_Verosimilitud" ----
plotdir=paste0(dirname.R0.profile, "/plots_Verosimilitud")
dir.create(path=plotdir,
           showWarnings = TRUE, 
           recursive = TRUE)
# 4. Crear un subdirectorio llamado "simple" ----
reference.dir <- paste0(dirname.R0.profile,'/simple') 
dir.create(path=reference.dir, showWarnings = TRUE, recursive = TRUE)
# 5. Copiar el resultado del modelo base completo en este directorio ----
file.copy(Sys.glob(paste(dirname.model.run, "*.*", sep="/"),
                   dirmark = FALSE),
                    reference.dir)
# 6. Leer la salida del modelo base ----
Base <- SS_output(dir=reference.dir,covar=T)
# 7. Copiar los archivos necesarios de "simple" al directorio "Perfil_Verosimilitud" ----
copy_SS_inputs(dir.old = reference.dir, 
               dir.new =  dirname.R0.profile,
               copy_exe = TRUE,
               verbose = FALSE)
# 8. Leer los archivos del modelo ----
inputs <- r4ss::SS_read(dir = dirname.R0.profile)
# 9. Editar el archivo control la fase de estimación recdev ----
inputs$ctl$recdev_phase <- 1
# 10. Editar el archivo starter para leer los valores de inicio ----
inputs$start$init_values_src <- 0
# 11. Vector de valores para el perfil ----
R0.vec <- seq(18,30,1)  
Nprof.R0 <- length(R0.vec)
# 12. Cambiar el nombre del archivo control en el archivo starter.ss ----
inputs$start$ctlfile <- "control_modified.ss" 
# 13. Incluir prior_like para parámetros no estimados ----
inputs$start$prior_like <- 1                                 
# 14. Escribir los modelos modificados ----
r4ss::SS_write(inputs, dir = dirname.R0.profile, overwrite = TRUE)
# 15. Ejecutar la función profile() ----
#?SS_profile()
profile <- profile(dir=dirname.R0.profile, # directory
                      exe="ss_osx",
                      oldctlfile ="control.ss",
                      newctlfile="control_modified.ss",
                      string="SR_LN(R0)",
                      profilevec=R0.vec)
# 16. Leer los archivos de salida ----
# (con nombres como Report1.sso, Report2.sso, etc.)
prof.R0.models <- SSgetoutput(dirvec=dirname.R0.profile, 
                              keyvec=1:Nprof.R0, 
                              getcovar = FALSE) 
# 17. Resumir las salidas con la función SSsummarize()  ----
prof.R0.summary <- SSsummarize(prof.R0.models)
# 18. Identificar los componentes de Verosimilitud ----
mainlike_components         <- c('TOTAL',
                                 "Survey", 
                                 'Length_comp',
                                 "Age_comp",
                                 "Catch",
                                 'Size_at_age',
                                 'Recruitment') 
mainlike_components_labels  <- c('Total likelihood',
                                 'Index likelihood',
                                 'Length likelihood',
                                 "Age likelihood",
                                 "Catch Likelihood",
                                 'Size_at_age likelihood',
                                 'Recruitment likelihood') 

```


### `SSplotProfile()`

```{r eval=FALSE, echo = FALSE}
png(file.path(plotdir,"R0_profile_plot.png"),
    width=7,
    height=4.5,
    res=300,
    units='in')
par(mar=c(5,4,1,1))
SSplotProfile(prof.R0.summary,           # summary object
              profile.string = "R0",     # substring of profile parameter
              profile.label=expression(log(italic(R)[0])), 
              ymax=2050,minfraction = 0.001,
              pheight=4.5, 
              print=FALSE, 
              plotdir=plotdir, 
              components = mainlike_components, 
              component.labels = mainlike_components_labels,
              add_cutoff = TRUE,
              cutoff_prob = 0.95)

Baseval <- round(Base$parameters$Value[grep("R0",Base$parameters$Label)],2)
Baselab <- paste(Baseval,sep="")
axis(1,at=Baseval,label=Baselab)
abline(v = Baseval, lty=2)
dev.off()
```

### Convergence Criteria

The convergence criterion used for model calibration is set to a final threshold of **0.0001** (or equivalently **1.0e-04**). This criterion defines the minimum acceptable difference between successive model iterations. Convergence is considered achieved when the absolute change in the objective function value or key parameters falls below this threshold. A smaller convergence value ensures that the model achieves a high degree of accuracy and stability in its final estimates, indicating that further iterations are unlikely to result in significant changes to the parameter estimates.


```{r eval=FALSE, echo = FALSE}
# Comparación de series de tiempo 
labs <- paste("SR_Ln(R0) = ",R0.vec)
labs[which(round(R0.vec,2)==Baseval)] <- paste("SR_Ln(R0) = ",
                                               Baseval,"(Base model)")

SSplotComparisons(prof.R0.summary,
                  legendlabels=labs,
                  pheight=4.5,png=TRUE,
                  plotdir=plotdir,
                  legendloc='bottomleft')


```

piner Plot

```{r eval = FALSE}
#### R0_profile_plot_Length_like ----
png(file.path(plotdir,"R0_profile_plot_Length_like.png"),
    width=7,
    height=4.5,
    res=300,
    units='in')
par(mar=c(5,4,1,1))
PinerPlot(prof.R0.summary, 
          profile.string = "R0", 
          component = "Length_like",
          main = "Changes in length-composition likelihoods by fleet",
          add_cutoff = TRUE,
          cutoff_prob = 0.95)
Baseval <- round(Base$parameters$Value[grep("SR_LN",
                                      Base$parameters$Label)],2)
Baselab <- paste(Baseval,sep="")
axis(1,at=Baseval,
     label=Baselab)
abline(v = Baseval, lty=2)
dev.off()
```


```{r eval=FALSE}
#### R0_profile_plot_Survey_like ----
png(file.path(plotdir,"R0_profile_plot_Survey_like.png"),
    width=7,
    height=4.5,
    res=300,
    units='in')
par(mar=c(5,4,1,1))
PinerPlot(prof.R0.summary, 
          profile.string = "R0", 
          component = "Surv_like",
          main = "Changes in Index likelihoods by fleet",
          add_cutoff = TRUE,
          cutoff_prob = 0.95, legendloc="topleft")
Baseval <- round(Base$parameters$Value[grep("SR_LN",
                                            Base$parameters$Label)],2)
Baselab <- paste(Baseval,sep="")
axis(1,at=Baseval,label=Baselab)
abline(v = Baseval, lty=2)
dev.off()
```
## Outputs

### Outputs Model `s1.1`

```{r}
outps1 <- base.model1.1$timeseries[1:37, 2:8]

# addWorksheet(wb, "variable_s2")
# writeData(wb, "variable_s2", outps1)
# 
# # Guardar el workbook
# saveWorkbook(wb, "DataKrill.xlsx", overwrite = TRUE)
# 
# 
# out_s2 <- read_excel("DataKrill.xlsx", 
#     sheet = "variable_s2")
outps1 %>%
  kbl(booktabs = TRUE,
      format = "html",
    caption = "Main variables outputs from stock asssessment krill in WAP `s1.1`") %>%
  kable_paper("hover", 
              full_width = TRUE)%>%
  kable_styling(latex_options = c("striped",
                                  "condensed"),
                full_width = FALSE,
                font_size=9,
                html_font = "arial")#%>% 
  #pack_rows(index = c("Estimation" = 1,
   #                     "Prediction" = 45))
```
### Outputs Model `s1.2`

```{r}
outps2 <- base.model1.2$timeseries[1:37, 2:8]

# addWorksheet(wb, "variable_s2")
# writeData(wb, "variable_s2", outps1)
# 
# # Guardar el workbook
# saveWorkbook(wb, "DataKrill.xlsx", overwrite = TRUE)
# 
# 
# out_s2 <- read_excel("DataKrill.xlsx", 
#     sheet = "variable_s2")
outps2 %>%
  kbl(booktabs = TRUE,
      format = "html",
    caption = "Main variables outputs from stock asssessment krill in WAP in `s1.2`") %>%
  kable_paper("hover", 
              full_width = TRUE)%>%
  kable_styling(latex_options = c("striped",
                                  "condensed"),
                full_width = FALSE,
                font_size=9,
                html_font = "arial")#%>% 
  #pack_rows(index = c("Estimation" = 1,
   #                     "Prediction" = 45))
```



### Outputs Model `s1.3`

```{r}
outps3 <- base.model1.3$timeseries[1:37, 2:8]

# addWorksheet(wb, "variable_s2")
# writeData(wb, "variable_s2", outps1)
# 
# # Guardar el workbook
# saveWorkbook(wb, "DataKrill.xlsx", overwrite = TRUE)
# 
# 
# out_s2 <- read_excel("DataKrill.xlsx", 
#     sheet = "variable_s2")
outps3 %>%
  kbl(booktabs = TRUE,
      format = "html",
    caption = "Main variables outputs from stock asssessment krill in WAP in `s1.3`") %>%
  kable_paper("hover", 
              full_width = TRUE)%>%
  kable_styling(latex_options = c("striped",
                                  "condensed"),
                full_width = FALSE,
                font_size=9,
                html_font = "arial")#%>% 
  #pack_rows(index = c("Estimation" = 1,
   #                     "Prediction" = 45))
```

### Outputs Model `s1.4`

```{r}
outps4 <- base.model1.3$timeseries[1:37, 2:8]

# addWorksheet(wb, "variable_s2")
# writeData(wb, "variable_s2", outps1)
# 
# # Guardar el workbook
# saveWorkbook(wb, "DataKrill.xlsx", overwrite = TRUE)
# 
# 
# out_s2 <- read_excel("DataKrill.xlsx", 
#     sheet = "variable_s2")
outps4 %>%
  kbl(booktabs = TRUE,
      format = "html",
    caption = "Main variables outputs from stock asssessment krill in WAP in `s1.4`") %>%
  kable_paper("hover", 
              full_width = TRUE)%>%
  kable_styling(latex_options = c("striped",
                                  "condensed"),
                full_width = FALSE,
                font_size=9,
                html_font = "arial")#%>% 
  #pack_rows(index = c("Estimation" = 1,
   #                     "Prediction" = 45))
```


## Comparison

### Comparison outputs betwwen scenarios

```{r}

#read in all model runs
#note if cover=T you need a hessian; if covar=F you do not need a hessian
biglist <- SSgetoutput(keyvec = NULL,
                       dirvec = c(dir1.1,
                                  dir1.2,
                                  dir1.3,
                                  dir1.4),
                       getcovar = F)

#create summary of model runs from list above
summaryoutputall <- SSsummarize(biglist)
```

```{r}
comtable <- SStableComparisons(summaryoutputall,
                   likenames = c("TOTAL", 
                                 "Survey", 
                                 "Length_comp",
                                 "Age_comp", 
                                 "priors",
                                 "Size_at_age"), 
                   names = c("Recr_Virgin",
                             "R0", 
                             "steep",
                             "NatM",
                             "L_at_Amax", 
                             "VonBert_K", 
                             "SSB_Virg", 
                             "Bratio_2020",
                             "SPRratio_2020"),
                   digits = NULL,
                   modelnames = c("s1.1", "s1.2",
                                  "s1.3",
                                  "s1.4"),
                   csv = F,
                   csvdir =~"/DOCAS/SA_Krill",
                   csvfile = "parameter_comparison_table.csv",
                 verbose = TRUE,
                   mcmc = FALSE)

kbl(comtable, booktabs = T,format = "latex",
    caption = "Comparision s1.1, s1.2, s1.3, s1.4")  %>% 
    kable_styling(latex_options = "scale_down")

```

```{r eval=FALSE}
#read in all model runs
#note if cover=T you need a hessian; if covar=F you do not need a hessian
biglist1_4 <- SSgetoutput(keyvec = NULL,
                       dirvec = c(
                                  dir1.1,
                                  dir1.2,
                                  dir1.3,
                                  dir1.4),
                       getcovar = F)

#create summary of model runs from list above
summaryoutput1_4 <- SSsummarize(biglist1_4)

SSplotComparisons(summaryoutput1_4,
                  legendlabels = c("Ref Model: No Env-Predator",
                 "S1.1 w/ Predator data",
                 "S1.1 w/ Env data",
                 "S1.1 w/ Env and Predator data"),
                 filenameprefix = "COM1",
                 labels = c("Year", 
                            "Spawning biomass (t)",
                            "Relative spawning biomass", 
                            "Age-0 recruits (1,000s)",
                            "Recruitment deviations", 
                            "Index", "Log index", 
                            "1 - SPR", 
                            "Density",
                            "Management target", 
                            "Minimum stock size threshold",
                            "Spawning output",
                            "Harvest rate"),
                 png=TRUE,
                 plotdir=Figs)
```

Comparison between select models `Ref Model: No Env-Predator` and `S1.1 w/ Env and Predator data`

```{r eval=FALSE}
#read in all model runs
#note if cover=T you need a hessian; if covar=F you do not need a hessian
biglist14 <- SSgetoutput(keyvec = NULL,
                       dirvec = c(
                                  dir1.1,
                                  dir1.4),
                       getcovar = F)

#create summary of model runs from list above
summaryoutput14 <- SSsummarize(biglist14)

comp14 <- SSplotComparisons(summaryoutput14,
                  legendlabels = c("Ref Model: No Env-Predator",
                 "S1.1 w/ Env and Predator data"),
                  filenameprefix = "COM2",
                 labels = c("Year", 
                            "Spawning biomass (t)",
                            "Relative spawning biomass", 
                            "Age-0 recruits (1,000s)",
                            "Recruitment deviations", 
                            "Index", "Log index", 
                            "1 - SPR", 
                            "Density",
                            "Management target", 
                            "Minimum stock size threshold",
                            "Spawning output",
                            "Harvest rate"),
                 png=TRUE,plotdir=Figs)
```

### Comparsion in sd long term time series

```{r fig.height=6, fig.width=8}

sblt1 <-  base.model1.1$timeseries$Bio_all
sblt2 <-  base.model1.2$timeseries$Bio_all
sblt3 <-  base.model1.3$timeseries$Bio_all
sblt4 <-  base.model1.4$timeseries$Bio_all

errt <- as.data.frame(cbind(sblt1,
              sblt2,
              sblt3,
              sblt4))
err_long <- errt %>%
  pivot_longer(cols = everything(),
               names_to = "variable",
               values_to = "value")  
titles <- c("Ref Model: No Env-Predator",
                 "S1.1 w/ Predator data",
                 "S1.1 w/ Env data",
                 "S1.1 w/ Env and Predator data")
ggplot(err_long, aes(x = variable, y = value)) +
  geom_boxplot(width=0.5) +
  geom_jitter(width = 0.2, alpha = 0.3) +
  labs(title = "Long Term Predictions",
       x = "",
       y = "Spawning Biomass (t)") +
  theme_few() +
  scale_x_discrete(labels= titles)+
  theme(axis.text.x = element_text(angle = 90,
                                   hjust = 1))
```

### Autocorrelation in Recruit 

To evaluate the temporal correlation structure of krill recruitment under different model configurations, we performed an Autocorrelation Function (ACF) analysis. The ACF measures the correlation between observations at different time lags, helping to assess whether recruitment estimates exhibit persistence or randomness across time.

The analysis was conducted on recruitment estimates derived from four model scenarios:

A reference model without environmental or predator influences (Ref Model: No Env-Predator).
A model incorporating predator data (S1.1 w/ Predator data).
A model incorporating environmental data (S1.1 w/ Env data).
A model incorporating both environmental and predator data (S1.1 w/ Env and Predator data).
Each model's residuals were extracted, and the autocorrelation function (ACF) was computed for a time lag range of up to 15 years. The dashed blue lines in the plots represent the 95% confidence intervals, indicating the threshold beyond which correlation values are statistically significant. If autocorrelation values remain within this range, it suggests that the recruitment estimates behave as a random process with no significant dependence on past values. Conversely, autocorrelation values exceeding these bounds indicate recruitment persistence or cyclic patterns.

```{r fig.width=5, fig.height=7}
recs1 <- base.model1.1$recruit$pred_recr
recs2 <- base.model1.2$recruit$pred_recr
recs3 <- base.model1.3$recruit$pred_recr
recs4 <- base.model1.4$recruit$pred_recr

p1 <- ggAcf(base.model1.1$recruit$pred_recr) +
  ggtitle("Ref Model: No Env-Predator") +
  theme_few()
p2 <- ggAcf(base.model1.2$recruit$pred_recr) +
  ggtitle("S1.1 w/ Predator data") +
  theme_few()
p3 <- ggAcf(base.model1.3$recruit$pred_recr) +
  ggtitle("S1.1 w/ Env data") +
  theme_few()
p4 <- ggAcf(base.model1.4$recruit$pred_recr) +
  ggtitle("S1.1 w/ Env and Predator data") +
  theme_few()

ggarrange(p1,
          p2,
          p3,
          p4, ncol=1, nrow = 4,
          common.legend = T)

```

The ACF plots indicate that the reference model (without environmental or predator data) exhibits weak but noticeable positive autocorrelation at certain lags, suggesting some degree of recruitment dependence over time. However, this autocorrelation does not appear strong or systematic.

The model incorporating only predator data shows a slight reduction in autocorrelation magnitude, suggesting that predator-driven recruitment variability may have captured part of the temporal structure in the data.

The model with only environmental data exhibits a further reduction in autocorrelation, implying that environmental variability explains a larger portion of recruitment trends than predator effects alone.

Finally, the model that includes both environmental and predator data presents the lowest autocorrelation values, with nearly all bars remaining within the confidence bounds. This suggests that incorporating both factors provides the most effective explanation of recruitment fluctuations, reducing unexplained temporal structure.

Overall, these results indicate that recruitment variability is at least partially driven by environmental and predator influences, and models integrating these factors provide more robust and independent recruitment estimates, minimizing systematic dependencies over time.

### Recruit deviation

```{r fig.width=5, fig.height=7}
dev1 <- base.model1.1$recruit[8:35,c(1,7)]%>% 
  mutate(Serie = "s1.1")
dev2 <- base.model1.2$recruit[8:35,c(1,7)] %>% 
  mutate(Serie = "s1.2")
dev3 <- base.model1.3$recruit[8:35,c(1,7)] %>% 
  mutate(Serie = "s1.3")
dev4 <- base.model1.4$recruit[8:35,c(1,7)] %>% 
  mutate(Serie = "s1.4")


data_list <- list(dev1, 
                  dev2,
                  dev3,
                  dev4)
titles <- c("Ref Model: No Env-Predator",
                 "S1.1 w/ Predator data",
                 "S1.1 w/ Env data",
                 "S1.1 w/ Env and Predator data")
# Lista para almacenar los gráficos
plots <- list()

for (i in 1:length(data_list)) {
  p <- ggplot(data_list[[i]], aes(x = Yr, y = dev)) +
    geom_point() +
    stat_smooth(method = "loess", 
                span = 0.15, 
                se = T) +
    geom_hline(yintercept = 0,
               col="red",
               linetype = "dotdash")+
    ggtitle(titles[i]) +
    theme(axis.text.x = element_text(angle = 90,
                                   hjust = 1))+
    ylim(-2,2)+
    theme_few()
  plots[[i]] <- p
}

# Organizar los gráficos con ggarrange
combined_plot <- ggarrange(plotlist = plots, ncol = 1, nrow = 4)

combined_plot
```



```{r}

df1 <- base.model1.1$likelihoods_used %>%
  select(values) %>%
  mutate(Componente = rownames(base.model1.1$likelihoods_used), Modelo = "s1.1")

df2 <- base.model1.2$likelihoods_used %>%
  select(values) %>%
  mutate(Componente = rownames(base.model1.2$likelihoods_used), Modelo = "s1.2")

df3 <- base.model1.3$likelihoods_used %>%
  select(values) %>%
  mutate(Componente = rownames(base.model1.3$likelihoods_used), Modelo = "s1.3")

df4 <- base.model1.4$likelihoods_used %>%
  select(values) %>%
  mutate(Componente = rownames(base.model1.4$likelihoods_used), Modelo = "s1.4")

df <- bind_rows(df1, 
                df2,
                df3,
                df4)

ggplot(df, aes(x = Componente, y = values, fill = Modelo)) +
  geom_col(position = position_dodge(), width = 0.7) +  
  theme_few() +
  coord_flip() +  
  scale_fill_manual(name= "Scenario",
                    values = c("s1.1" = '#ca0020',
                               "s1.2" = '#f4a582', 
                               "s1.3" = '#bababa',
                               "s1.4" = '#404040')) + 
  labs(x = "",
       y = "Log-Normal Likelihood", 
       title = "") 
```

### Platoons analisis

```{r}
# Definir los valores del eje x
x_values <- seq(0, 7, by=0.1)

# Definir las medias y desviaciones estándar para los valores específicos
means <- c( 2.9,  3.5, 4.1)
std_devs <- c(0.65, 0.8, 0.65)


# Crear un dataframe que contenga los valores de x, la media y las desviaciones estándar
data <- data.frame(
  x = rep(x_values, each = length(means)),
  mean = rep(means, times = length(x_values)),
  std_dev = rep(std_devs, times = length(x_values))
)

# Calcular las curvas de las desviaciones estándar
data <- data %>%
  mutate(y = exp(-((x - mean)^2) / (2 * std_dev^2)))

# Graficar con ggplot
ggplot(data, aes(x = x, y = y, linetype = as.factor(mean))) +
  geom_line(size = 1) +
  scale_linetype_manual(values = means,
                        name ="Platoon") +
  labs(x = "", y = "growth increment") +
  theme_minimal()+
  xlim(0,7)

```
### AKL

In a catch-at-length model like krill assessment the AKL matrix is modelled trought parametrization process

```{r AKL, out.width='60%',  fig.cap="Representation of ALK Matrix to krill in 48.1"}
alk_matrix <- base.model1.1$ALK[,,1] # Ajusta las dimensiones según tu matriz

# Convertir la matriz en un data.frame
alk_df <- as.data.frame(as.table(alk_matrix))

# Renombrar columnas para mayor claridad
colnames(alk_df) <- c("Length", "TrueAge", "Value")

# Asegúrate de que las columnas Length y TrueAge sean numéricas
alk_df$Length <- as.numeric(as.character(alk_df$Length))
alk_df$TrueAge <- as.numeric(as.character(alk_df$TrueAge))

# Asegúrate de que la columna Value sea numérica
alk_df$Value <- as.numeric(alk_df$Value)

# Crear el plot usando ggplot
ggplot(alk_df, aes(x = TrueAge, y = Length, fill = Value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  labs(x = "True Age",
       y = "Length",
       fill = "Value") +
  theme_bw()+
  theme(legend.position = "none")
```


\newpage

# REFERENCES
